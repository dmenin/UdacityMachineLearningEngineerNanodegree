{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Working on Session data...\n",
      "0 from 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "print('Loading raw data...')\n",
    "train_users_path='C:\\\\git\\\\Airbnb\\\\_raw_data\\\\train_users_2.csv'\n",
    "test_users_path='C:\\\\git\\\\Airbnb\\\\_raw_data\\\\test_users.csv'\n",
    "sessions_path='C:\\\\git\\\\Airbnb\\\\_raw_data\\\\sessions.csv'\n",
    "\n",
    "#Note: age_gender_bkts.csv and countries.csv files are not used.\n",
    "\n",
    "#########Loading data#############\n",
    "#train_users\n",
    "df_train = pd.read_csv(train_users_path)\n",
    "target = df_train['country_destination']\n",
    "df_train = df_train.drop(['country_destination'], axis=1)\n",
    "\n",
    "#test_users\n",
    "df_test = pd.read_csv(test_users_path)    \n",
    "id_test = df_test['id']\n",
    "\n",
    "#sessions\n",
    "df_sessions = pd.read_csv(sessions_path)\n",
    "df_sessions['id'] = df_sessions['user_id']\n",
    "df_sessions = df_sessions.drop(['user_id'],axis=1)\n",
    "\n",
    "#########Preparing Session data########\n",
    "print('Working on Session data...')\n",
    "#Filling nan with specific value ('NAN')\n",
    "df_sessions.action = df_sessions.action.fillna('NAN')\n",
    "df_sessions.action_type = df_sessions.action_type.fillna('NAN')\n",
    "df_sessions.action_detail = df_sessions.action_detail.fillna('NAN')\n",
    "df_sessions.device_type = df_sessions.device_type.fillna('NAN')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Action values with low frequency are changed to 'OTHER'\n",
    "act_freq = 100  #Threshold for frequency\n",
    "#The single star * unpacks the sequence/collection into positional arguments\n",
    "act = dict(zip(*np.unique(df_sessions.action, return_counts=True)))\n",
    "df_sessions.action = df_sessions.action.apply(lambda x: 'OTHER' if act[x] < act_freq else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Computing value_counts. These are going to be used in the one-hot encoding\n",
    "#based feature generation (following loop).\n",
    "f_act = df_sessions.action.value_counts().argsort()\n",
    "f_act_detail = df_sessions.action_detail.value_counts().argsort()\n",
    "f_act_type = df_sessions.action_type.value_counts().argsort()\n",
    "f_dev_type = df_sessions.device_type.value_counts().argsort()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#grouping session by id. We will compute features from all rows with the same id.\n",
    "dgr_sess = df_sessions.groupby(['id'])\n",
    "#dgr_sess =df_sessions[df_sessions.id=='4grx6yxeby'].groupby(['id'])\n",
    "dgr_sess =df_sessions[:500].groupby(['id'])\n",
    "\n",
    "    \n",
    "#Loop on dgr_sess to create all the features.\n",
    "samples = []\n",
    "cont = 0\n",
    "ln = len(dgr_sess)\n",
    "\n",
    "for g in dgr_sess:\n",
    "    if cont%10000 == 0:\n",
    "        print(\"%s from %s\" %(cont, ln))\n",
    "    gr = g[1]\n",
    "    l = []\n",
    "    \n",
    "    #the id\n",
    "    l.append(g[0])\n",
    "    \n",
    "    #The actual first feature is the number of values.\n",
    "    l.append(len(gr))\n",
    "    \n",
    "    sev = gr.secs_elapsed.fillna(0).values   #These values are used later.\n",
    "    \n",
    "    #action features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act = [0] * len(f_act)\n",
    "    for i,v in enumerate(gr.action.values):\n",
    "        c_act[f_act[v]] += 1\n",
    "        \n",
    "    _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "    c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "    l = l + c_act\n",
    "    \n",
    "    #action_detail features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act_detail = [0] * len(f_act_detail)\n",
    "    for i,v in enumerate(gr.action_detail.values):\n",
    "        c_act_detail[f_act_detail[v]] += 1 \n",
    "    _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "    c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "    l = l + c_act_detail\n",
    "    \n",
    "    #action_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std\n",
    "    #+ log of the sum of secs_elapsed for each value)\n",
    "    l_act_type = [0] * len(f_act_type)\n",
    "    c_act_type = [0] * len(f_act_type)\n",
    "    for i,v in enumerate(gr.action_type.values):\n",
    "        l_act_type[f_act_type[v]] += sev[i]   \n",
    "        c_act_type[f_act_type[v]] += 1  \n",
    "    l_act_type = np.log(1 + np.array(l_act_type)).tolist()\n",
    "    _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "    c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "    l = l + c_act_type + l_act_type    \n",
    "    \n",
    "    #device_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_dev_type  = [0] * len(f_dev_type)\n",
    "    for i,v in enumerate(gr.device_type .values):\n",
    "        c_dev_type[f_dev_type[v]] += 1 \n",
    "    c_dev_type.append(len(np.unique(gr.device_type.values)))\n",
    "    _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "    c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]        \n",
    "    l = l + c_dev_type    \n",
    "    \n",
    "    #secs_elapsed features        \n",
    "    l_secs = [0] * 5 \n",
    "    l_log = [0] * 15\n",
    "    if len(sev) > 0:\n",
    "        #Simple statistics about the secs_elapsed values.\n",
    "        l_secs[0] = np.log(1 + np.sum(sev))\n",
    "        l_secs[1] = np.log(1 + np.mean(sev)) \n",
    "        l_secs[2] = np.log(1 + np.std(sev))\n",
    "        l_secs[3] = np.log(1 + np.median(sev))\n",
    "        l_secs[4] = l_secs[0] / float(l[1])\n",
    "        \n",
    "        #Values are grouped in 15 intervals. Compute the number of values\n",
    "        #in each interval.\n",
    "        log_sev = np.log(1 + sev).astype(int)\n",
    "        l_log = np.bincount(log_sev, minlength=15).tolist()                      \n",
    "    l = l + l_secs + l_log\n",
    "    \n",
    "    #The list l has the feature values of one sample.\n",
    "    samples.append(l)\n",
    "    cont += 1\n",
    "    \n",
    "col_names = []    #name of the columns\n",
    "for i in range(len(samples[0])-1):\n",
    "    col_names.append('c_' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = np.array(samples)\n",
    "samp_ar = samples[:, 1:].astype(np.float16)\n",
    "samp_id = samples[:, 0]   #The first element in obs is the id of the sample.\n",
    "\n",
    "df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "df_agg_sess['id'] = samp_id\n",
    "df_agg_sess.index = df_agg_sess.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########Working on train and test data#####################\n",
    "print('Working on users data...')\n",
    "df_tt = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_tt.index = df_tt.id\n",
    "df_tt = df_tt.fillna(-1)  #Inputing this kind of missing value with -1 (missing values in train and test)\n",
    "df_tt = df_tt.replace('-unknown-', -1) #-unknown is another way of missing value, then = -1.\n",
    "\n",
    "########Creating features for train+test\n",
    "#Removing date_first_booking\n",
    "df_tt = df_tt.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "#Number of nulls\n",
    "df_tt['n_null'] = np.array([sum(r == -1) for r in df_tt.values])\n",
    "dac = np.vstack(df_tt.date_account_created.astype(str).apply(lambda x: list(map(int,x.split('-')))).values)\n",
    "\n",
    "#date_account_created\n",
    "#(Computing year, month, day, week_number, weekday)\n",
    "df_tt['dac_y'] = dac[:,0]\n",
    "df_tt['dac_m'] = dac[:,1]\n",
    "df_tt['dac_d'] = dac[:,2]\n",
    "dac_dates = [datetime(x[0],x[1],x[2]) for x in dac]\n",
    "df_tt['dac_wn'] = np.array([d.isocalendar()[1] for d in dac_dates])\n",
    "df_tt['dac_w'] = np.array([d.weekday() for d in dac_dates])\n",
    "\n",
    "df_tt_wd = pd.get_dummies(df_tt.dac_w, prefix='dac_w')\n",
    "df_tt = df_tt.drop(['date_account_created', 'dac_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timestamp_first_active\n",
    "#(Computing year, month, day, hour, week_number, weekday)\n",
    "tfa = np.vstack(df_tt.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "df_tt['tfa_y'] = tfa[:,0]\n",
    "df_tt['tfa_m'] = tfa[:,1]\n",
    "df_tt['tfa_d'] = tfa[:,2]\n",
    "df_tt['tfa_h'] = tfa[:,3]\n",
    "tfa_dates = [datetime(x[0],x[1],x[2],x[3],x[4],x[5]) for x in tfa]\n",
    "df_tt['tfa_wn'] = np.array([d.isocalendar()[1] for d in tfa_dates])\n",
    "df_tt['tfa_w'] = np.array([d.weekday() for d in tfa_dates])\n",
    "df_tt_wd = pd.get_dummies(df_tt.tfa_w, prefix='tfa_w')\n",
    "df_tt = df_tt.drop(['timestamp_first_active', 'tfa_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timespans between dates\n",
    "#(Computing absolute number of seconds of difference between dates, sign of the difference)\n",
    "df_tt['dac_tfa_secs'] = np.array([np.log(1+abs((dac_dates[i]-tfa_dates[i]).total_seconds())) for i in range(len(dac_dates))])\n",
    "df_tt['sig_dac_tfa'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).total_seconds()) for i in range(len(dac_dates))])\n",
    "\n",
    "#Comptute seasons from dates\n",
    "#(Computing the season for the two dates)\n",
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "           (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "           (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "           (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "           (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "def get_season(dt):\n",
    "    dt = dt.date()\n",
    "    dt = dt.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= dt <= end)\n",
    "\n",
    "df_tt['season_dac'] = np.array([get_season(dt) for dt in dac_dates])\n",
    "df_tt['season_tfa'] = np.array([get_season(dt) for dt in tfa_dates])\n",
    "\n",
    "#Age\n",
    "#(Keeping ages in 14 < age < 99 as OK and grouping others according different kinds of mistakes)\n",
    "av = df_tt.age.values\n",
    "av = np.where(np.logical_and(av<2000, av>1900), 2014-av, av) #This are birthdays instead of age (estimating age by doing 2014 - value)\n",
    "av = np.where(np.logical_and(av<14, av>0), 4, av) #Using specific value=4 for age values below 14\n",
    "av = np.where(np.logical_and(av<2016, av>2010), 9, av) #This is the current year insted of age (using specific value = 9)\n",
    "av = np.where(av > 99, 110, av)  #Using specific value=110 for age values above 99\n",
    "df_tt['age'] = av\n",
    "\n",
    "\n",
    "#AgeRange\n",
    "#(One-hot encoding of the edge according these intervals)\n",
    "interv =  [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100]\n",
    "def get_interv_value(age):\n",
    "    iv = 20\n",
    "    for i in range(len(interv)):\n",
    "        if age < interv[i]:\n",
    "            iv = i \n",
    "            break\n",
    "    return iv\n",
    "\n",
    "df_tt['age_interv'] = df_tt.age.apply(lambda x: get_interv_value(x))\n",
    "df_tt_ai = pd.get_dummies(df_tt.age_interv, prefix='age_interv')\n",
    "df_tt = df_tt.drop(['age_interv'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_ai), axis=1)\n",
    "\n",
    "#One-hot-encoding features\n",
    "ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', \n",
    "             'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "for f in ohe_feats:\n",
    "    df_tt_dummy = pd.get_dummies(df_tt[f], prefix=f)\n",
    "    df_tt = df_tt.drop([f], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_dummy), axis=1)  \n",
    "\n",
    "######Merging train-test with session data#################\n",
    "df_all = pd.merge(df_tt, df_agg_sess, how='left')\n",
    "df_all = df_all.drop(['id'], axis=1)\n",
    "df_all = df_all.fillna(-2)  #Missing features for samples without sesssion data.\n",
    "df_all['all_null'] = np.array([sum(r<0) for r in df_all.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# piv_train = len(target)\n",
    "# vals = df_all.values\n",
    "# le = LabelEncoder()\n",
    "# X = vals[:piv_train]\n",
    "# y = le.fit_transform(target.values)\n",
    "# X_test = vals[piv_train:]\n",
    "# print('Shape X = %s, Shape X_test = %s'%(X.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_all.to_pickle('C:/git/Airbnb/other_solutions/sandro/df_all.pkl')\n",
    "pickle.dump(id_test, open('C:/git/Airbnb/other_solutions/sandro/id_test.pkl', 'wb'))\n",
    "pickle.dump(target, open('C:/git/Airbnb/other_solutions/sandro/target.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X = (213451L, 661L), Shape X_test = (62096L, 661L)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
