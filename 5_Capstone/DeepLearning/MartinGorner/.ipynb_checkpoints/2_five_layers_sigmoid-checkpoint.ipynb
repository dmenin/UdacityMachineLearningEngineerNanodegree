{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/train-images-idx3-ubyte.mnist\n",
      "Loading data/train-labels-idx1-ubyte.mnist\n",
      "Loading data/t10k-images-idx3-ubyte.mnist\n",
      "Loading data/t10k-labels-idx1-ubyte.mnist\n"
     ]
    }
   ],
   "source": [
    "import mnist_data\n",
    "import tensorflow as tf\n",
    "import tensorflowvisu\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# neural network with 5 layers\n",
    "#\n",
    "# · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28\n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/    -- fully connected layer (sigmoid)      W1 [784, 200]      B1[200]\n",
    "#  · · · · · · · · ·                                             Y1 [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/      -- fully connected layer (sigmoid)      W2 [200, 100]      B2[100]\n",
    "#    · · · · · · ·                                               Y2 [batch, 100]\n",
    "#    \\x/x\\x/x\\x/         -- fully connected layer (sigmoid)      W3 [100, 60]       B3[60]\n",
    "#     · · · · ·                                                  Y3 [batch, 60]\n",
    "#     \\x/x\\x/            -- fully connected layer (sigmoid)      W4 [60, 30]        B4[30]\n",
    "#      · · ·                                                     Y4 [batch, 30]\n",
    "#      \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n",
    "#       ·                                                        Y5 [batch, 10]\n",
    "\n",
    "# Download images and labels\n",
    "mnist = mnist_data.read_data_sets(\"data\")\n",
    "\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])# correct answers will go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
    "L = 200\n",
    "M = 100\n",
    "N = 60\n",
    "O = 30\n",
    "# Weights initialised with small random values between -0.2 and +0.2\n",
    "# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([M]))\n",
    "W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([N]))\n",
    "W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "B4 = tf.Variable(tf.zeros([O]))\n",
    "W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# The model\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + B3)\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + B4)\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib visualisation\n",
    "allweights = tf.concat(0, [tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])])\n",
    "allbiases  = tf.concat(0, [tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])])\n",
    "I = tensorflowvisu.tf_format_mnist_images(X, Y, Y_)\n",
    "It = tensorflowvisu.tf_format_mnist_images(X, Y, Y_, 1000, lines=25)\n",
    "datavis = tensorflowvisu.MnistDataVis()\n",
    "\n",
    "# training step, learning rate = 0.003\n",
    "learning_rate = 0.003\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# init\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.14 loss: 230.908 (lr:0.003)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1135 test loss: 232.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda2/lib/python2.7/site-packages/matplotlib/backend_bases.py:2437: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: accuracy:0.11 loss: 228.51 (lr:0.003)\n",
      "20: ********* epoch 1 ********* test accuracy:0.1028 test loss: 228.213\n",
      "40: accuracy:0.27 loss: 208.438 (lr:0.003)\n",
      "40: ********* epoch 1 ********* test accuracy:0.3057 test loss: 208.046\n",
      "60: accuracy:0.44 loss: 168.352 (lr:0.003)\n",
      "60: ********* epoch 1 ********* test accuracy:0.4587 test loss: 171.818\n",
      "80: accuracy:0.6 loss: 142.84 (lr:0.003)\n",
      "80: ********* epoch 1 ********* test accuracy:0.5208 test loss: 146.546\n",
      "100: accuracy:0.45 loss: 140.873 (lr:0.003)\n",
      "100: ********* epoch 1 ********* test accuracy:0.55 test loss: 132.904\n",
      "120: accuracy:0.6 loss: 120.657 (lr:0.003)\n",
      "140: accuracy:0.52 loss: 116.244 (lr:0.003)\n",
      "160: accuracy:0.5 loss: 130.201 (lr:0.003)\n",
      "180: accuracy:0.69 loss: 89.6941 (lr:0.003)\n",
      "200: accuracy:0.61 loss: 97.075 (lr:0.003)\n",
      "200: ********* epoch 1 ********* test accuracy:0.6957 test loss: 92.7701\n",
      "220: accuracy:0.8 loss: 68.7736 (lr:0.003)\n",
      "240: accuracy:0.72 loss: 80.1196 (lr:0.003)\n",
      "260: accuracy:0.78 loss: 69.8215 (lr:0.003)\n",
      "280: accuracy:0.92 loss: 47.2195 (lr:0.003)\n",
      "300: accuracy:0.73 loss: 73.1189 (lr:0.003)\n",
      "300: ********* epoch 1 ********* test accuracy:0.8102 test loss: 67.6152\n",
      "320: accuracy:0.83 loss: 63.2349 (lr:0.003)\n",
      "340: accuracy:0.81 loss: 57.7668 (lr:0.003)\n",
      "360: accuracy:0.84 loss: 51.3452 (lr:0.003)\n",
      "380: accuracy:0.89 loss: 43.5699 (lr:0.003)\n",
      "400: accuracy:0.89 loss: 43.4734 (lr:0.003)\n",
      "400: ********* epoch 1 ********* test accuracy:0.8981 test loss: 45.9897\n",
      "420: accuracy:0.92 loss: 35.9285 (lr:0.003)\n",
      "440: accuracy:0.89 loss: 31.9215 (lr:0.003)\n",
      "460: accuracy:0.89 loss: 36.9833 (lr:0.003)\n",
      "480: accuracy:0.91 loss: 40.2739 (lr:0.003)\n",
      "500: accuracy:0.9 loss: 38.7411 (lr:0.003)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9182 test loss: 36.0605\n",
      "520: accuracy:0.93 loss: 41.8684 (lr:0.003)\n",
      "540: accuracy:0.88 loss: 46.012 (lr:0.003)\n",
      "560: accuracy:0.94 loss: 29.2457 (lr:0.003)\n",
      "580: accuracy:0.95 loss: 18.7572 (lr:0.003)\n",
      "600: accuracy:0.94 loss: 41.4708 (lr:0.003)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9228 test loss: 32.606\n",
      "620: accuracy:0.94 loss: 31.9851 (lr:0.003)\n",
      "640: accuracy:0.94 loss: 21.0194 (lr:0.003)\n",
      "660: accuracy:0.95 loss: 20.5352 (lr:0.003)\n",
      "680: accuracy:0.97 loss: 22.0892 (lr:0.003)\n",
      "700: accuracy:0.88 loss: 47.5448 (lr:0.003)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9383 test loss: 25.2627\n",
      "720: accuracy:0.93 loss: 29.0981 (lr:0.003)\n",
      "740: accuracy:0.94 loss: 21.0571 (lr:0.003)\n",
      "760: accuracy:0.94 loss: 16.6529 (lr:0.003)\n",
      "780: accuracy:0.9 loss: 41.255 (lr:0.003)\n",
      "800: accuracy:0.93 loss: 30.7583 (lr:0.003)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9428 test loss: 23.3516\n",
      "820: accuracy:0.97 loss: 14.6539 (lr:0.003)\n",
      "840: accuracy:0.93 loss: 32.1118 (lr:0.003)\n",
      "860: accuracy:0.93 loss: 19.8366 (lr:0.003)\n",
      "880: accuracy:0.88 loss: 36.7198 (lr:0.003)\n",
      "900: accuracy:0.95 loss: 24.6647 (lr:0.003)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9429 test loss: 22.4927\n",
      "920: accuracy:0.94 loss: 27.3119 (lr:0.003)\n",
      "940: accuracy:0.93 loss: 25.4984 (lr:0.003)\n",
      "960: accuracy:0.97 loss: 13.8861 (lr:0.003)\n",
      "980: accuracy:0.96 loss: 15.7831 (lr:0.003)\n",
      "1000: accuracy:0.96 loss: 20.545 (lr:0.003)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9451 test loss: 20.5187\n",
      "1020: accuracy:0.97 loss: 17.9398 (lr:0.003)\n",
      "1040: accuracy:0.99 loss: 9.96705 (lr:0.003)\n",
      "1060: accuracy:0.97 loss: 11.7921 (lr:0.003)\n",
      "1080: accuracy:0.96 loss: 15.9724 (lr:0.003)\n",
      "1100: accuracy:0.97 loss: 9.75351 (lr:0.003)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9558 test loss: 17.5206\n",
      "1120: accuracy:0.98 loss: 7.23638 (lr:0.003)\n",
      "1140: accuracy:0.96 loss: 19.2521 (lr:0.003)\n",
      "1160: accuracy:0.95 loss: 15.4518 (lr:0.003)\n",
      "1180: accuracy:0.96 loss: 10.1227 (lr:0.003)\n",
      "1200: accuracy:0.97 loss: 11.7645 (lr:0.003)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9532 test loss: 18.3882\n",
      "1220: accuracy:0.96 loss: 19.521 (lr:0.003)\n",
      "1240: accuracy:0.99 loss: 4.89878 (lr:0.003)\n",
      "1260: accuracy:0.94 loss: 19.0364 (lr:0.003)\n",
      "1280: accuracy:0.98 loss: 13.093 (lr:0.003)\n",
      "1300: accuracy:0.95 loss: 22.9141 (lr:0.003)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9581 test loss: 16.2769\n",
      "1320: accuracy:0.99 loss: 6.22834 (lr:0.003)\n",
      "1340: accuracy:0.99 loss: 4.56015 (lr:0.003)\n",
      "1360: accuracy:0.94 loss: 21.9388 (lr:0.003)\n",
      "1380: accuracy:0.98 loss: 9.31571 (lr:0.003)\n",
      "1400: accuracy:1.0 loss: 4.20419 (lr:0.003)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9522 test loss: 18.8526\n",
      "1420: accuracy:0.92 loss: 24.1642 (lr:0.003)\n",
      "1440: accuracy:0.97 loss: 18.6342 (lr:0.003)\n",
      "1460: accuracy:0.98 loss: 14.682 (lr:0.003)\n",
      "1480: accuracy:0.94 loss: 21.4667 (lr:0.003)\n",
      "1500: accuracy:0.98 loss: 10.1802 (lr:0.003)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9619 test loss: 14.4583\n",
      "1520: accuracy:0.97 loss: 12.3971 (lr:0.003)\n",
      "1540: accuracy:0.97 loss: 11.5165 (lr:0.003)\n",
      "1560: accuracy:0.98 loss: 9.70088 (lr:0.003)\n",
      "1580: accuracy:0.96 loss: 15.3911 (lr:0.003)\n",
      "1600: accuracy:0.97 loss: 12.5796 (lr:0.003)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9628 test loss: 14.2527\n",
      "1620: accuracy:0.97 loss: 16.0389 (lr:0.003)\n",
      "1640: accuracy:0.97 loss: 14.6787 (lr:0.003)\n",
      "1660: accuracy:0.99 loss: 4.31809 (lr:0.003)\n",
      "1680: accuracy:0.97 loss: 11.8269 (lr:0.003)\n",
      "1700: accuracy:0.98 loss: 7.6018 (lr:0.003)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9642 test loss: 13.4068\n",
      "1720: accuracy:0.97 loss: 11.6468 (lr:0.003)\n",
      "1740: accuracy:0.96 loss: 20.4233 (lr:0.003)\n",
      "1760: accuracy:0.94 loss: 19.1169 (lr:0.003)\n",
      "1780: accuracy:0.94 loss: 15.4192 (lr:0.003)\n",
      "1800: accuracy:0.97 loss: 8.13269 (lr:0.003)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9628 test loss: 13.8302\n",
      "1820: accuracy:1.0 loss: 2.48598 (lr:0.003)\n",
      "1840: accuracy:0.99 loss: 3.92597 (lr:0.003)\n",
      "1860: accuracy:0.99 loss: 5.52716 (lr:0.003)\n",
      "1880: accuracy:0.99 loss: 4.62149 (lr:0.003)\n",
      "1900: accuracy:0.98 loss: 7.60357 (lr:0.003)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9639 test loss: 13.0992\n",
      "1920: accuracy:0.95 loss: 21.6394 (lr:0.003)\n",
      "1940: accuracy:0.98 loss: 7.23985 (lr:0.003)\n",
      "1960: accuracy:0.99 loss: 6.07046 (lr:0.003)\n",
      "1980: accuracy:0.97 loss: 8.71387 (lr:0.003)\n",
      "2000: accuracy:0.96 loss: 16.5923 (lr:0.003)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9657 test loss: 12.9839\n",
      "2020: accuracy:0.97 loss: 6.14723 (lr:0.003)\n",
      "2040: accuracy:0.98 loss: 7.02197 (lr:0.003)\n",
      "2060: accuracy:0.98 loss: 7.10463 (lr:0.003)\n",
      "2080: accuracy:1.0 loss: 2.80592 (lr:0.003)\n",
      "2100: accuracy:0.98 loss: 12.9196 (lr:0.003)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9645 test loss: 13.5471\n",
      "2120: accuracy:1.0 loss: 3.35162 (lr:0.003)\n",
      "2140: accuracy:0.98 loss: 8.8399 (lr:0.003)\n",
      "2160: accuracy:0.98 loss: 6.62734 (lr:0.003)\n",
      "2180: accuracy:0.98 loss: 10.2944 (lr:0.003)\n",
      "2200: accuracy:0.99 loss: 8.21957 (lr:0.003)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9692 test loss: 11.8417\n",
      "2220: accuracy:0.98 loss: 6.04668 (lr:0.003)\n",
      "2240: accuracy:0.97 loss: 9.07808 (lr:0.003)\n",
      "2260: accuracy:1.0 loss: 3.45513 (lr:0.003)\n",
      "2280: accuracy:0.96 loss: 19.4843 (lr:0.003)\n",
      "2300: accuracy:0.98 loss: 8.71824 (lr:0.003)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9675 test loss: 12.13\n",
      "2320: accuracy:0.95 loss: 17.6427 (lr:0.003)\n",
      "2340: accuracy:0.97 loss: 9.52661 (lr:0.003)\n",
      "2360: accuracy:0.96 loss: 17.1504 (lr:0.003)\n",
      "2380: accuracy:0.98 loss: 9.11944 (lr:0.003)\n",
      "2400: accuracy:1.0 loss: 2.06815 (lr:0.003)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.967 test loss: 11.8215\n",
      "2420: accuracy:0.98 loss: 5.22675 (lr:0.003)\n",
      "2440: accuracy:0.98 loss: 6.66485 (lr:0.003)\n",
      "2460: accuracy:0.96 loss: 12.164 (lr:0.003)\n",
      "2480: accuracy:0.99 loss: 6.21741 (lr:0.003)\n",
      "2500: accuracy:1.0 loss: 1.68723 (lr:0.003)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9681 test loss: 11.8774\n",
      "2520: accuracy:0.96 loss: 13.5809 (lr:0.003)\n",
      "2540: accuracy:0.98 loss: 6.69513 (lr:0.003)\n",
      "2560: accuracy:0.93 loss: 13.4906 (lr:0.003)\n",
      "2580: accuracy:0.99 loss: 2.86516 (lr:0.003)\n",
      "2600: accuracy:1.0 loss: 1.66529 (lr:0.003)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9701 test loss: 10.7349\n",
      "2620: accuracy:0.98 loss: 5.36324 (lr:0.003)\n",
      "2640: accuracy:0.99 loss: 3.24668 (lr:0.003)\n",
      "2660: accuracy:1.0 loss: 0.704328 (lr:0.003)\n",
      "2680: accuracy:0.96 loss: 14.9901 (lr:0.003)\n",
      "2700: accuracy:0.99 loss: 8.63123 (lr:0.003)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9712 test loss: 10.7053\n",
      "2720: accuracy:1.0 loss: 1.98301 (lr:0.003)\n",
      "2740: accuracy:0.98 loss: 8.23858 (lr:0.003)\n",
      "2760: accuracy:0.98 loss: 6.59945 (lr:0.003)\n",
      "2780: accuracy:0.99 loss: 9.51356 (lr:0.003)\n",
      "2800: accuracy:0.97 loss: 7.19605 (lr:0.003)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.97 test loss: 11.0488\n",
      "2820: accuracy:0.97 loss: 14.4066 (lr:0.003)\n",
      "2840: accuracy:0.97 loss: 10.7191 (lr:0.003)\n",
      "2860: accuracy:0.95 loss: 11.3163 (lr:0.003)\n",
      "2880: accuracy:1.0 loss: 0.683442 (lr:0.003)\n",
      "2900: accuracy:0.98 loss: 9.02229 (lr:0.003)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9684 test loss: 11.3861\n",
      "2920: accuracy:0.99 loss: 3.41107 (lr:0.003)\n",
      "2940: accuracy:0.98 loss: 4.06005 (lr:0.003)\n",
      "2960: accuracy:1.0 loss: 2.23478 (lr:0.003)\n",
      "2980: accuracy:0.98 loss: 6.86243 (lr:0.003)\n",
      "3000: accuracy:0.97 loss: 13.1011 (lr:0.003)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9674 test loss: 11.591\n",
      "3020: accuracy:1.0 loss: 1.6545 (lr:0.003)\n",
      "3040: accuracy:0.98 loss: 4.55082 (lr:0.003)\n",
      "3060: accuracy:0.99 loss: 2.60071 (lr:0.003)\n",
      "3080: accuracy:0.99 loss: 9.02825 (lr:0.003)\n",
      "3100: accuracy:0.99 loss: 3.19873 (lr:0.003)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9721 test loss: 10.5208\n",
      "3120: accuracy:0.99 loss: 4.18536 (lr:0.003)\n",
      "3140: accuracy:0.98 loss: 8.63134 (lr:0.003)\n",
      "3160: accuracy:0.99 loss: 3.79224 (lr:0.003)\n",
      "3180: accuracy:0.96 loss: 6.29816 (lr:0.003)\n",
      "3200: accuracy:0.97 loss: 14.5905 (lr:0.003)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9724 test loss: 10.375\n",
      "3220: accuracy:0.98 loss: 7.91351 (lr:0.003)\n",
      "3240: accuracy:0.99 loss: 2.43821 (lr:0.003)\n",
      "3260: accuracy:0.99 loss: 5.14936 (lr:0.003)\n",
      "3280: accuracy:0.99 loss: 3.33091 (lr:0.003)\n",
      "3300: accuracy:0.96 loss: 5.42779 (lr:0.003)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9729 test loss: 10.3648\n",
      "3320: accuracy:0.98 loss: 3.19241 (lr:0.003)\n",
      "3340: accuracy:0.99 loss: 2.61788 (lr:0.003)\n",
      "3360: accuracy:1.0 loss: 1.4143 (lr:0.003)\n",
      "3380: accuracy:1.0 loss: 1.20807 (lr:0.003)\n",
      "3400: accuracy:0.98 loss: 8.46971 (lr:0.003)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9729 test loss: 10.4967\n",
      "3420: accuracy:0.99 loss: 5.45736 (lr:0.003)\n",
      "3440: accuracy:0.97 loss: 6.02958 (lr:0.003)\n",
      "3460: accuracy:0.97 loss: 8.46337 (lr:0.003)\n",
      "3480: accuracy:0.99 loss: 3.20491 (lr:0.003)\n",
      "3500: accuracy:0.98 loss: 9.72765 (lr:0.003)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9728 test loss: 10.2567\n",
      "3520: accuracy:1.0 loss: 1.96322 (lr:0.003)\n",
      "3540: accuracy:0.99 loss: 3.93182 (lr:0.003)\n",
      "3560: accuracy:0.97 loss: 9.51595 (lr:0.003)\n",
      "3580: accuracy:0.99 loss: 5.75081 (lr:0.003)\n",
      "3600: accuracy:0.99 loss: 2.84849 (lr:0.003)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9698 test loss: 10.9714\n",
      "3620: accuracy:0.98 loss: 5.18557 (lr:0.003)\n",
      "3640: accuracy:1.0 loss: 1.25004 (lr:0.003)\n",
      "3660: accuracy:0.99 loss: 2.61965 (lr:0.003)\n",
      "3680: accuracy:1.0 loss: 0.891271 (lr:0.003)\n",
      "3700: accuracy:0.99 loss: 2.97928 (lr:0.003)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9741 test loss: 9.96498\n",
      "3720: accuracy:1.0 loss: 1.69574 (lr:0.003)\n",
      "3740: accuracy:0.99 loss: 8.104 (lr:0.003)\n",
      "3760: accuracy:0.98 loss: 14.2421 (lr:0.003)\n",
      "3780: accuracy:0.96 loss: 10.3674 (lr:0.003)\n",
      "3800: accuracy:0.99 loss: 5.01242 (lr:0.003)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9713 test loss: 11.4688\n",
      "3820: accuracy:0.99 loss: 2.83189 (lr:0.003)\n",
      "3840: accuracy:1.0 loss: 1.32355 (lr:0.003)\n",
      "3860: accuracy:0.99 loss: 1.93011 (lr:0.003)\n",
      "3880: accuracy:0.98 loss: 6.27973 (lr:0.003)\n",
      "3900: accuracy:1.0 loss: 0.523464 (lr:0.003)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9732 test loss: 10.5263\n",
      "3920: accuracy:0.98 loss: 3.18743 (lr:0.003)\n",
      "3940: accuracy:1.0 loss: 1.46859 (lr:0.003)\n",
      "3960: accuracy:0.97 loss: 9.41052 (lr:0.003)\n",
      "3980: accuracy:0.99 loss: 3.20375 (lr:0.003)\n",
      "4000: accuracy:0.98 loss: 3.53687 (lr:0.003)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9717 test loss: 10.56\n",
      "4020: accuracy:1.0 loss: 0.446691 (lr:0.003)\n",
      "4040: accuracy:0.98 loss: 3.52238 (lr:0.003)\n",
      "4060: accuracy:0.97 loss: 7.73921 (lr:0.003)\n",
      "4080: accuracy:1.0 loss: 0.958758 (lr:0.003)\n",
      "4100: accuracy:0.98 loss: 6.72374 (lr:0.003)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9693 test loss: 12.008\n",
      "4120: accuracy:0.97 loss: 14.4353 (lr:0.003)\n",
      "4140: accuracy:1.0 loss: 0.885152 (lr:0.003)\n",
      "4160: accuracy:0.98 loss: 3.75964 (lr:0.003)\n",
      "4180: accuracy:1.0 loss: 0.760436 (lr:0.003)\n",
      "4200: accuracy:1.0 loss: 0.685022 (lr:0.003)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9725 test loss: 9.8964\n",
      "4220: accuracy:1.0 loss: 1.88687 (lr:0.003)\n",
      "4240: accuracy:0.99 loss: 3.41819 (lr:0.003)\n",
      "4260: accuracy:1.0 loss: 0.774933 (lr:0.003)\n",
      "4280: accuracy:0.98 loss: 4.45734 (lr:0.003)\n",
      "4300: accuracy:1.0 loss: 0.839296 (lr:0.003)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9695 test loss: 12.3565\n",
      "4320: accuracy:1.0 loss: 0.890098 (lr:0.003)\n",
      "4340: accuracy:0.99 loss: 9.39452 (lr:0.003)\n",
      "4360: accuracy:1.0 loss: 1.56781 (lr:0.003)\n",
      "4380: accuracy:0.99 loss: 4.24326 (lr:0.003)\n",
      "4400: accuracy:0.96 loss: 13.377 (lr:0.003)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9706 test loss: 10.7601\n",
      "4420: accuracy:1.0 loss: 1.30708 (lr:0.003)\n",
      "4440: accuracy:0.98 loss: 3.67518 (lr:0.003)\n",
      "4460: accuracy:1.0 loss: 0.655051 (lr:0.003)\n",
      "4480: accuracy:1.0 loss: 1.06332 (lr:0.003)\n",
      "4500: accuracy:0.99 loss: 3.37385 (lr:0.003)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9735 test loss: 10.5877\n",
      "4520: accuracy:1.0 loss: 1.10272 (lr:0.003)\n",
      "4540: accuracy:0.98 loss: 4.70945 (lr:0.003)\n",
      "4560: accuracy:1.0 loss: 0.640519 (lr:0.003)\n",
      "4580: accuracy:1.0 loss: 1.36894 (lr:0.003)\n",
      "4600: accuracy:0.98 loss: 7.44476 (lr:0.003)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9753 test loss: 9.52647\n",
      "4620: accuracy:0.98 loss: 5.87681 (lr:0.003)\n",
      "4640: accuracy:1.0 loss: 1.47148 (lr:0.003)\n",
      "4660: accuracy:1.0 loss: 0.596659 (lr:0.003)\n",
      "4680: accuracy:1.0 loss: 0.67478 (lr:0.003)\n",
      "4700: accuracy:0.99 loss: 4.05485 (lr:0.003)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.969 test loss: 12.1201\n",
      "4720: accuracy:0.98 loss: 6.87137 (lr:0.003)\n",
      "4740: accuracy:1.0 loss: 0.624338 (lr:0.003)\n",
      "4760: accuracy:0.99 loss: 7.46264 (lr:0.003)\n",
      "4780: accuracy:0.96 loss: 8.26615 (lr:0.003)\n",
      "4800: accuracy:0.99 loss: 1.95195 (lr:0.003)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9715 test loss: 11.4261\n",
      "4820: accuracy:1.0 loss: 1.49205 (lr:0.003)\n",
      "4840: accuracy:1.0 loss: 0.718607 (lr:0.003)\n",
      "4860: accuracy:0.98 loss: 3.04777 (lr:0.003)\n",
      "4880: accuracy:1.0 loss: 0.412079 (lr:0.003)\n",
      "4900: accuracy:0.99 loss: 1.20748 (lr:0.003)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9759 test loss: 9.66156\n",
      "4920: accuracy:1.0 loss: 1.10239 (lr:0.003)\n",
      "4940: accuracy:0.98 loss: 2.82453 (lr:0.003)\n",
      "4960: accuracy:1.0 loss: 0.496529 (lr:0.003)\n",
      "4980: accuracy:0.99 loss: 1.37437 (lr:0.003)\n",
      "5000: accuracy:0.97 loss: 12.2638 (lr:0.003)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9724 test loss: 10.8525\n",
      "5020: accuracy:0.99 loss: 1.79777 (lr:0.003)\n",
      "5040: accuracy:0.98 loss: 6.49545 (lr:0.003)\n",
      "5060: accuracy:1.0 loss: 0.336263 (lr:0.003)\n",
      "5080: accuracy:0.99 loss: 1.47014 (lr:0.003)\n",
      "5100: accuracy:0.99 loss: 2.65535 (lr:0.003)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9739 test loss: 10.3507\n",
      "5120: accuracy:0.97 loss: 12.513 (lr:0.003)\n",
      "5140: accuracy:0.99 loss: 2.23651 (lr:0.003)\n",
      "5160: accuracy:0.99 loss: 1.78064 (lr:0.003)\n",
      "5180: accuracy:0.99 loss: 2.14851 (lr:0.003)\n",
      "5200: accuracy:0.99 loss: 2.55235 (lr:0.003)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9752 test loss: 10.0446\n",
      "5220: accuracy:0.99 loss: 4.43358 (lr:0.003)\n",
      "5240: accuracy:0.99 loss: 2.0701 (lr:0.003)\n",
      "5260: accuracy:0.98 loss: 5.68246 (lr:0.003)\n",
      "5280: accuracy:1.0 loss: 1.44757 (lr:0.003)\n",
      "5300: accuracy:1.0 loss: 0.553761 (lr:0.003)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.975 test loss: 10.2308\n",
      "5320: accuracy:1.0 loss: 0.402681 (lr:0.003)\n",
      "5340: accuracy:1.0 loss: 1.91873 (lr:0.003)\n",
      "5360: accuracy:1.0 loss: 0.546917 (lr:0.003)\n",
      "5380: accuracy:0.99 loss: 2.14267 (lr:0.003)\n",
      "5400: accuracy:0.99 loss: 3.80665 (lr:0.003)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.974 test loss: 10.8895\n",
      "5420: accuracy:0.99 loss: 5.52148 (lr:0.003)\n",
      "5440: accuracy:0.98 loss: 3.54132 (lr:0.003)\n",
      "5460: accuracy:0.99 loss: 5.19554 (lr:0.003)\n",
      "5480: accuracy:1.0 loss: 0.773111 (lr:0.003)\n",
      "5500: accuracy:1.0 loss: 0.723505 (lr:0.003)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9747 test loss: 9.91387\n",
      "5520: accuracy:1.0 loss: 0.510437 (lr:0.003)\n",
      "5540: accuracy:1.0 loss: 0.721915 (lr:0.003)\n",
      "5560: accuracy:1.0 loss: 0.172313 (lr:0.003)\n",
      "5580: accuracy:1.0 loss: 0.275442 (lr:0.003)\n",
      "5600: accuracy:0.99 loss: 1.72547 (lr:0.003)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9737 test loss: 10.4826\n",
      "5620: accuracy:0.99 loss: 3.32834 (lr:0.003)\n",
      "5640: accuracy:1.0 loss: 2.21724 (lr:0.003)\n",
      "5660: accuracy:1.0 loss: 1.26873 (lr:0.003)\n",
      "5680: accuracy:1.0 loss: 0.190289 (lr:0.003)\n",
      "5700: accuracy:0.99 loss: 3.41698 (lr:0.003)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9749 test loss: 10.308\n",
      "5720: accuracy:0.99 loss: 2.76384 (lr:0.003)\n",
      "5740: accuracy:1.0 loss: 1.27654 (lr:0.003)\n",
      "5760: accuracy:1.0 loss: 0.691144 (lr:0.003)\n",
      "5780: accuracy:0.98 loss: 5.57152 (lr:0.003)\n",
      "5800: accuracy:0.99 loss: 10.3641 (lr:0.003)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9745 test loss: 10.7634\n",
      "5820: accuracy:1.0 loss: 0.390842 (lr:0.003)\n",
      "5840: accuracy:0.97 loss: 7.77835 (lr:0.003)\n",
      "5860: accuracy:0.99 loss: 5.22852 (lr:0.003)\n",
      "5880: accuracy:1.0 loss: 0.914138 (lr:0.003)\n",
      "5900: accuracy:1.0 loss: 0.589042 (lr:0.003)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9735 test loss: 11.1292\n",
      "5920: accuracy:0.99 loss: 3.28425 (lr:0.003)\n",
      "5940: accuracy:0.99 loss: 2.86264 (lr:0.003)\n",
      "5960: accuracy:0.99 loss: 5.27968 (lr:0.003)\n",
      "5980: accuracy:0.99 loss: 3.69531 (lr:0.003)\n",
      "6000: accuracy:1.0 loss: 0.160944 (lr:0.003)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9754 test loss: 10.5401\n",
      "6020: accuracy:0.99 loss: 1.55612 (lr:0.003)\n",
      "6040: accuracy:1.0 loss: 0.416771 (lr:0.003)\n",
      "6060: accuracy:0.99 loss: 2.36402 (lr:0.003)\n",
      "6080: accuracy:0.99 loss: 6.31559 (lr:0.003)\n",
      "6100: accuracy:1.0 loss: 0.436498 (lr:0.003)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9693 test loss: 12.8631\n",
      "6120: accuracy:0.99 loss: 5.8588 (lr:0.003)\n",
      "6140: accuracy:0.98 loss: 4.78592 (lr:0.003)\n",
      "6160: accuracy:0.98 loss: 5.31057 (lr:0.003)\n",
      "6180: accuracy:1.0 loss: 0.618938 (lr:0.003)\n",
      "6200: accuracy:1.0 loss: 0.611532 (lr:0.003)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9678 test loss: 13.159\n",
      "6220: accuracy:1.0 loss: 0.559944 (lr:0.003)\n",
      "6240: accuracy:0.99 loss: 6.90988 (lr:0.003)\n",
      "6260: accuracy:0.99 loss: 6.06827 (lr:0.003)\n",
      "6280: accuracy:0.99 loss: 3.01983 (lr:0.003)\n",
      "6300: accuracy:1.0 loss: 0.912057 (lr:0.003)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9743 test loss: 11.0147\n",
      "6320: accuracy:0.97 loss: 13.6604 (lr:0.003)\n",
      "6340: accuracy:1.0 loss: 0.210114 (lr:0.003)\n",
      "6360: accuracy:1.0 loss: 0.860387 (lr:0.003)\n",
      "6380: accuracy:1.0 loss: 0.736138 (lr:0.003)\n",
      "6400: accuracy:0.99 loss: 2.68906 (lr:0.003)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9709 test loss: 12.3222\n",
      "6420: accuracy:1.0 loss: 1.03514 (lr:0.003)\n",
      "6440: accuracy:1.0 loss: 1.51225 (lr:0.003)\n",
      "6460: accuracy:0.98 loss: 10.336 (lr:0.003)\n",
      "6480: accuracy:1.0 loss: 0.632652 (lr:0.003)\n",
      "6500: accuracy:0.99 loss: 4.12116 (lr:0.003)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9748 test loss: 9.91521\n",
      "6520: accuracy:1.0 loss: 0.470728 (lr:0.003)\n",
      "6540: accuracy:0.98 loss: 3.38798 (lr:0.003)\n",
      "6560: accuracy:1.0 loss: 0.421906 (lr:0.003)\n",
      "6580: accuracy:0.99 loss: 6.34429 (lr:0.003)\n",
      "6600: accuracy:0.99 loss: 4.3827 (lr:0.003)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9756 test loss: 9.68214\n",
      "6620: accuracy:0.99 loss: 2.07924 (lr:0.003)\n",
      "6640: accuracy:1.0 loss: 0.221535 (lr:0.003)\n",
      "6660: accuracy:1.0 loss: 0.313819 (lr:0.003)\n",
      "6680: accuracy:1.0 loss: 0.524701 (lr:0.003)\n",
      "6700: accuracy:1.0 loss: 0.279867 (lr:0.003)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9765 test loss: 10.2767\n",
      "6720: accuracy:1.0 loss: 0.559353 (lr:0.003)\n",
      "6740: accuracy:1.0 loss: 0.237439 (lr:0.003)\n",
      "6760: accuracy:1.0 loss: 0.211081 (lr:0.003)\n",
      "6780: accuracy:0.99 loss: 0.95981 (lr:0.003)\n",
      "6800: accuracy:1.0 loss: 0.244327 (lr:0.003)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9757 test loss: 10.4789\n",
      "6820: accuracy:1.0 loss: 0.386546 (lr:0.003)\n",
      "6840: accuracy:0.99 loss: 1.75405 (lr:0.003)\n",
      "6860: accuracy:1.0 loss: 0.991379 (lr:0.003)\n",
      "6880: accuracy:0.98 loss: 10.9554 (lr:0.003)\n",
      "6900: accuracy:1.0 loss: 1.20309 (lr:0.003)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9707 test loss: 12.1438\n",
      "6920: accuracy:0.99 loss: 4.15034 (lr:0.003)\n",
      "6940: accuracy:0.98 loss: 7.13581 (lr:0.003)\n",
      "6960: accuracy:0.98 loss: 7.53794 (lr:0.003)\n",
      "6980: accuracy:0.96 loss: 9.29427 (lr:0.003)\n",
      "7000: accuracy:0.99 loss: 3.42089 (lr:0.003)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9745 test loss: 10.3694\n",
      "7020: accuracy:0.99 loss: 1.56304 (lr:0.003)\n",
      "7040: accuracy:1.0 loss: 0.423498 (lr:0.003)\n",
      "7060: accuracy:0.99 loss: 2.21501 (lr:0.003)\n",
      "7080: accuracy:0.98 loss: 6.11366 (lr:0.003)\n",
      "7100: accuracy:0.98 loss: 3.45 (lr:0.003)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9764 test loss: 9.86819\n",
      "7120: accuracy:0.97 loss: 8.24657 (lr:0.003)\n",
      "7140: accuracy:0.97 loss: 4.58561 (lr:0.003)\n",
      "7160: accuracy:1.0 loss: 0.468422 (lr:0.003)\n",
      "7180: accuracy:0.98 loss: 4.6585 (lr:0.003)\n",
      "7200: accuracy:1.0 loss: 0.342191 (lr:0.003)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9747 test loss: 11.1125\n",
      "7220: accuracy:1.0 loss: 0.781453 (lr:0.003)\n",
      "7240: accuracy:1.0 loss: 0.733078 (lr:0.003)\n",
      "7260: accuracy:1.0 loss: 0.278971 (lr:0.003)\n",
      "7280: accuracy:1.0 loss: 0.375598 (lr:0.003)\n",
      "7300: accuracy:1.0 loss: 0.649321 (lr:0.003)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9748 test loss: 10.894\n",
      "7320: accuracy:0.99 loss: 3.85582 (lr:0.003)\n",
      "7340: accuracy:1.0 loss: 0.148471 (lr:0.003)\n",
      "7360: accuracy:0.99 loss: 1.94931 (lr:0.003)\n",
      "7380: accuracy:0.99 loss: 2.35334 (lr:0.003)\n",
      "7400: accuracy:1.0 loss: 0.134384 (lr:0.003)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9727 test loss: 12.0476\n",
      "7420: accuracy:1.0 loss: 0.369913 (lr:0.003)\n",
      "7440: accuracy:1.0 loss: 0.480538 (lr:0.003)\n",
      "7460: accuracy:1.0 loss: 0.320288 (lr:0.003)\n",
      "7480: accuracy:0.99 loss: 1.7347 (lr:0.003)\n",
      "7500: accuracy:1.0 loss: 0.849015 (lr:0.003)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9691 test loss: 14.5085\n",
      "7520: accuracy:0.97 loss: 8.65802 (lr:0.003)\n",
      "7540: accuracy:1.0 loss: 1.37001 (lr:0.003)\n",
      "7560: accuracy:0.99 loss: 5.2347 (lr:0.003)\n",
      "7580: accuracy:0.99 loss: 2.11312 (lr:0.003)\n",
      "7600: accuracy:0.98 loss: 5.77559 (lr:0.003)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9711 test loss: 12.7309\n",
      "7620: accuracy:1.0 loss: 0.171352 (lr:0.003)\n",
      "7640: accuracy:0.99 loss: 1.1928 (lr:0.003)\n",
      "7660: accuracy:1.0 loss: 0.151724 (lr:0.003)\n",
      "7680: accuracy:1.0 loss: 1.09286 (lr:0.003)\n",
      "7700: accuracy:1.0 loss: 1.13191 (lr:0.003)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9742 test loss: 11.3091\n",
      "7720: accuracy:0.99 loss: 3.79609 (lr:0.003)\n",
      "7740: accuracy:1.0 loss: 0.640987 (lr:0.003)\n",
      "7760: accuracy:0.99 loss: 2.85921 (lr:0.003)\n",
      "7780: accuracy:0.98 loss: 5.89678 (lr:0.003)\n",
      "7800: accuracy:0.99 loss: 0.814525 (lr:0.003)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9759 test loss: 11.0138\n",
      "7820: accuracy:0.99 loss: 1.37386 (lr:0.003)\n",
      "7840: accuracy:0.99 loss: 1.83573 (lr:0.003)\n",
      "7860: accuracy:1.0 loss: 0.315998 (lr:0.003)\n",
      "7880: accuracy:0.99 loss: 1.47657 (lr:0.003)\n",
      "7900: accuracy:1.0 loss: 0.155355 (lr:0.003)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9765 test loss: 10.4215\n",
      "7920: accuracy:1.0 loss: 0.103333 (lr:0.003)\n",
      "7940: accuracy:0.99 loss: 2.25226 (lr:0.003)\n",
      "7960: accuracy:1.0 loss: 0.699142 (lr:0.003)\n",
      "7980: accuracy:0.99 loss: 1.50251 (lr:0.003)\n",
      "8000: accuracy:1.0 loss: 1.04115 (lr:0.003)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9729 test loss: 11.6823\n",
      "8020: accuracy:0.99 loss: 2.48335 (lr:0.003)\n",
      "8040: accuracy:0.99 loss: 1.73174 (lr:0.003)\n",
      "8060: accuracy:1.0 loss: 0.411474 (lr:0.003)\n",
      "8080: accuracy:1.0 loss: 1.09072 (lr:0.003)\n",
      "8100: accuracy:1.0 loss: 0.558187 (lr:0.003)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9765 test loss: 10.3926\n",
      "8120: accuracy:0.99 loss: 4.35651 (lr:0.003)\n",
      "8140: accuracy:0.98 loss: 2.74276 (lr:0.003)\n",
      "8160: accuracy:1.0 loss: 0.179922 (lr:0.003)\n",
      "8180: accuracy:1.0 loss: 0.227467 (lr:0.003)\n",
      "8200: accuracy:1.0 loss: 0.264184 (lr:0.003)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9744 test loss: 11.3366\n",
      "8220: accuracy:1.0 loss: 0.450589 (lr:0.003)\n",
      "8240: accuracy:0.97 loss: 6.25907 (lr:0.003)\n",
      "8260: accuracy:0.98 loss: 4.6269 (lr:0.003)\n",
      "8280: accuracy:1.0 loss: 0.530612 (lr:0.003)\n",
      "8300: accuracy:0.99 loss: 3.74316 (lr:0.003)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9769 test loss: 10.3062\n",
      "8320: accuracy:1.0 loss: 0.374609 (lr:0.003)\n",
      "8340: accuracy:1.0 loss: 0.599523 (lr:0.003)\n",
      "8360: accuracy:1.0 loss: 1.40656 (lr:0.003)\n",
      "8380: accuracy:1.0 loss: 0.15987 (lr:0.003)\n",
      "8400: accuracy:0.99 loss: 2.4871 (lr:0.003)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.976 test loss: 10.6311\n",
      "8420: accuracy:1.0 loss: 0.35876 (lr:0.003)\n",
      "8440: accuracy:1.0 loss: 0.340244 (lr:0.003)\n",
      "8460: accuracy:0.99 loss: 1.17636 (lr:0.003)\n",
      "8480: accuracy:0.99 loss: 2.89435 (lr:0.003)\n",
      "8500: accuracy:1.0 loss: 0.182202 (lr:0.003)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9779 test loss: 10.4445\n",
      "8520: accuracy:1.0 loss: 0.710452 (lr:0.003)\n",
      "8540: accuracy:1.0 loss: 0.290358 (lr:0.003)\n",
      "8560: accuracy:1.0 loss: 0.113979 (lr:0.003)\n",
      "8580: accuracy:1.0 loss: 0.116252 (lr:0.003)\n",
      "8600: accuracy:0.99 loss: 1.13463 (lr:0.003)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9765 test loss: 11.07\n",
      "8620: accuracy:0.99 loss: 3.70808 (lr:0.003)\n",
      "8640: accuracy:1.0 loss: 0.222163 (lr:0.003)\n",
      "8660: accuracy:0.99 loss: 3.48175 (lr:0.003)\n",
      "8680: accuracy:0.99 loss: 1.60705 (lr:0.003)\n",
      "8700: accuracy:1.0 loss: 0.512936 (lr:0.003)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9767 test loss: 10.7674\n",
      "8720: accuracy:1.0 loss: 0.195969 (lr:0.003)\n",
      "8740: accuracy:0.99 loss: 5.29098 (lr:0.003)\n",
      "8760: accuracy:0.99 loss: 3.43742 (lr:0.003)\n",
      "8780: accuracy:1.0 loss: 0.802381 (lr:0.003)\n",
      "8800: accuracy:0.99 loss: 3.93281 (lr:0.003)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9769 test loss: 10.7842\n",
      "8820: accuracy:1.0 loss: 0.341043 (lr:0.003)\n",
      "8840: accuracy:1.0 loss: 0.611485 (lr:0.003)\n",
      "8860: accuracy:1.0 loss: 0.136145 (lr:0.003)\n",
      "8880: accuracy:1.0 loss: 0.339554 (lr:0.003)\n",
      "8900: accuracy:1.0 loss: 0.59448 (lr:0.003)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9764 test loss: 10.777\n",
      "8920: accuracy:1.0 loss: 0.116968 (lr:0.003)\n",
      "8940: accuracy:0.98 loss: 3.22629 (lr:0.003)\n",
      "8960: accuracy:0.99 loss: 6.65712 (lr:0.003)\n",
      "8980: accuracy:1.0 loss: 0.399238 (lr:0.003)\n",
      "9000: accuracy:0.99 loss: 4.62757 (lr:0.003)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9758 test loss: 10.8125\n",
      "9020: accuracy:0.99 loss: 5.4933 (lr:0.003)\n",
      "9040: accuracy:0.99 loss: 2.74196 (lr:0.003)\n",
      "9060: accuracy:0.98 loss: 4.59776 (lr:0.003)\n",
      "9080: accuracy:0.99 loss: 2.40434 (lr:0.003)\n",
      "9100: accuracy:0.99 loss: 2.70902 (lr:0.003)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9749 test loss: 11.3077\n",
      "9120: accuracy:1.0 loss: 0.912431 (lr:0.003)\n",
      "9140: accuracy:0.99 loss: 1.56389 (lr:0.003)\n",
      "9160: accuracy:1.0 loss: 0.485581 (lr:0.003)\n",
      "9180: accuracy:1.0 loss: 0.182916 (lr:0.003)\n",
      "9200: accuracy:1.0 loss: 0.204282 (lr:0.003)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9772 test loss: 11.237\n",
      "9220: accuracy:1.0 loss: 0.402817 (lr:0.003)\n",
      "9240: accuracy:0.99 loss: 1.33568 (lr:0.003)\n",
      "9260: accuracy:0.99 loss: 0.951407 (lr:0.003)\n",
      "9280: accuracy:1.0 loss: 0.142933 (lr:0.003)\n",
      "9300: accuracy:0.97 loss: 8.11721 (lr:0.003)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9702 test loss: 14.355\n",
      "9320: accuracy:1.0 loss: 0.108952 (lr:0.003)\n",
      "9340: accuracy:1.0 loss: 0.0434739 (lr:0.003)\n",
      "9360: accuracy:1.0 loss: 0.0964259 (lr:0.003)\n",
      "9380: accuracy:1.0 loss: 0.218577 (lr:0.003)\n",
      "9400: accuracy:0.99 loss: 6.10754 (lr:0.003)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9726 test loss: 11.7251\n",
      "9420: accuracy:1.0 loss: 0.133282 (lr:0.003)\n",
      "9440: accuracy:1.0 loss: 0.314088 (lr:0.003)\n",
      "9460: accuracy:0.98 loss: 3.82763 (lr:0.003)\n",
      "9480: accuracy:0.98 loss: 2.16218 (lr:0.003)\n",
      "9500: accuracy:1.0 loss: 0.311942 (lr:0.003)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9755 test loss: 11.6603\n",
      "9520: accuracy:1.0 loss: 0.981211 (lr:0.003)\n",
      "9540: accuracy:1.0 loss: 0.31178 (lr:0.003)\n",
      "9560: accuracy:1.0 loss: 0.490767 (lr:0.003)\n",
      "9580: accuracy:0.99 loss: 1.57116 (lr:0.003)\n",
      "9600: accuracy:0.99 loss: 2.14916 (lr:0.003)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9742 test loss: 11.8338\n",
      "9620: accuracy:0.94 loss: 20.1564 (lr:0.003)\n",
      "9640: accuracy:1.0 loss: 0.97261 (lr:0.003)\n",
      "9660: accuracy:0.99 loss: 5.58711 (lr:0.003)\n",
      "9680: accuracy:0.99 loss: 1.25726 (lr:0.003)\n",
      "9700: accuracy:0.99 loss: 4.6118 (lr:0.003)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9731 test loss: 12.5159\n",
      "9720: accuracy:1.0 loss: 0.150366 (lr:0.003)\n",
      "9740: accuracy:1.0 loss: 0.16193 (lr:0.003)\n",
      "9760: accuracy:1.0 loss: 0.330992 (lr:0.003)\n",
      "9780: accuracy:0.98 loss: 8.34267 (lr:0.003)\n",
      "9800: accuracy:0.98 loss: 2.85412 (lr:0.003)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9707 test loss: 13.361\n",
      "9820: accuracy:1.0 loss: 0.0835991 (lr:0.003)\n",
      "9840: accuracy:1.0 loss: 0.975389 (lr:0.003)\n",
      "9860: accuracy:1.0 loss: 0.168981 (lr:0.003)\n",
      "9880: accuracy:1.0 loss: 0.121603 (lr:0.003)\n",
      "9900: accuracy:1.0 loss: 0.19755 (lr:0.003)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9762 test loss: 10.5337\n",
      "9920: accuracy:1.0 loss: 0.725156 (lr:0.003)\n",
      "9940: accuracy:0.98 loss: 4.42679 (lr:0.003)\n",
      "9960: accuracy:1.0 loss: 0.559949 (lr:0.003)\n",
      "9980: accuracy:1.0 loss: 0.0699247 (lr:0.003)\n",
      "10001: accuracy:1.0 loss: 0.245819 (lr:0.003)\n",
      "10001: ********* epoch 17 ********* test accuracy:0.9763 test loss: 10.8041\n",
      "max test accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can call this function in a loop to train the model, 100 images at a time\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, im, w, b = sess.run([accuracy, cross_entropy, I, allweights, allbiases], {X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        datavis.append_training_curves_data(i, a, c)\n",
    "        datavis.update_image1(im)\n",
    "        datavis.append_data_histograms(i, w, b)\n",
    "\n",
    "    # compute test values for visualisation\n",
    "    if update_test_data:\n",
    "        a, c, im = sess.run([accuracy, cross_entropy, It], {X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        datavis.append_test_curves_data(i, a, c)\n",
    "        datavis.update_image2(im)\n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y})\n",
    "\n",
    "datavis.animate(training_step, iterations=10000+1, train_data_update_freq=20, test_data_update_freq=100, more_tests_at_start=True)\n",
    "\n",
    "# to save the animation as a movie, add save_movie=True as an argument to datavis.animate\n",
    "# to disable the visualisation use the following line instead of the datavis.animate line\n",
    "# for i in range(10000+1): training_step(i, i % 100 == 0, i % 20 == 0)\n",
    "\n",
    "print(\"max test accuracy: \" + str(datavis.get_max_test_accuracy()))\n",
    "\n",
    "# Some results to expect:\n",
    "# (In all runs, if sigmoids are used, all biases are initialised at 0, if RELUs are used,\n",
    "# all biases are initialised at 0.1 apart from the last one which is initialised at 0.)\n",
    "\n",
    "## learning rate = 0.003, 10K iterations\n",
    "# final test accuracy = 0.9788 (sigmoid - slow start, training cross-entropy not stabilised in the end)\n",
    "# final test accuracy = 0.9825 (relu - above 0.97 in the first 1500 iterations but noisy curves)\n",
    "\n",
    "## now with learning rate = 0.0001, 10K iterations\n",
    "# final test accuracy = 0.9722 (relu - slow but smooth curve, would have gone higher in 20K iterations)\n",
    "\n",
    "## decaying learning rate from 0.003 to 0.0001 decay_speed 2000, 10K iterations\n",
    "# final test accuracy = 0.9746 (sigmoid - training cross-entropy not stabilised)\n",
    "# final test accuracy = 0.9824 (relu - training set fully learned, test accuracy stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
