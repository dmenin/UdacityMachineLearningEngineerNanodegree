{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################  IMPORTS\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import sys\n",
    "#from tabulate import tabulate\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################  AUX FUNCTIONS:\n",
    "from operator import itemgetter as it\n",
    "#Does the same as unpivot_from_resultdf_to_dict, get_topn and pivot_back_with_topn TOGHETER\n",
    "#but Im keeping the functions above in case I need to include more logic that I cant on this one\n",
    "def get_top5_on_lis_of_labels(dfResult, submission):\n",
    "    lis_of_labels = []\n",
    "    for _, row in dfResult.iterrows():\n",
    "        if submission:\n",
    "            for option in (map(it(0), row[1:].order(ascending=0)[:5].iteritems())):\n",
    "                lis_of_labels.append([row['id'], option])\n",
    "        else:        \n",
    "            lis_of_labels.append(map(it(0), row[1:].order(ascending=0)[:5].iteritems()))\n",
    "    return lis_of_labels\n",
    "\n",
    "\n",
    "def get_top5_on_lis_of_labels2F(dfResult, submission):\n",
    "    list_of_labels = []    \n",
    "\n",
    "    for _, row in dfResult.iterrows():\n",
    "        l = []\n",
    "        done = 0\n",
    "        pred_country = 'error'\n",
    "        for option in (\n",
    "                        map (\n",
    "                            it(0,1), row[1:].order(ascending=0)[:5].iteritems()\n",
    "                            )\n",
    "                      ):\n",
    "            country = option[0]\n",
    "            prob = option[1]\n",
    "            if done == 0:\n",
    "                pred_country = country\n",
    "                #if prob >= 0.9: \n",
    "                #    pred_country = country\n",
    "                #    done = 1\n",
    "                #if prob <= 0.3:                 \n",
    "                #    pred_country = 'NDF'\n",
    "                #    done = 1\n",
    "            if submission:\n",
    "                list_of_labels.append([row['id'], pred_country]) #for submissions\n",
    "            else:\n",
    "                l.append(pred_country)   #for train CV\n",
    "        \n",
    "        if submission == False:\n",
    "            list_of_labels.append(l)\n",
    "            \n",
    "    return list_of_labels    \n",
    "\n",
    "\n",
    "\n",
    "#Unpivot from a dataframe where each prediction is a column to a dict where each prediciton is a row:\n",
    "def unpivot_from_resultdf_to_dict(dfResult):\n",
    "    result = {}\n",
    "    for key in dfResult:\n",
    "        if key != 'id':\n",
    "            for i in xrange(0,len(dfResult['id'])):\n",
    "                if dfResult['id'][i] not in result:\n",
    "                    result[dfResult['id'][i]] = []\n",
    "                result[dfResult['id'][i]] += [(key,dfResult[key][i])]\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_topn(result,n):\n",
    "    #Use this to get the top value for each option\n",
    "    return [x[0] for x in sorted(result,key=lambda x:-x[1])[0:min(len(result),n)]]\n",
    "\n",
    "\n",
    "def pivot_back_with_topn(result,n, submission,dfResult):\n",
    "    #keys = sorted([k for k in result])\n",
    "    #keys = ([k for k in result])\n",
    "    auxindex = []\n",
    "    for i in dfResult['id']:\n",
    "        auxindex.append(i)\n",
    "    \n",
    "    keys = ([k for k in auxindex])\n",
    "    list_of_labels = []\n",
    "    for key in keys:\n",
    "        if submission:\n",
    "            for option in get_topn(result[key],n):\n",
    "                #print str(key) + ',' + str(option)\n",
    "                list_of_labels.append([str(key), str(option)] )\n",
    "        else:\n",
    "            list_of_labels.append(get_topn(result[key],n))\n",
    "    return list_of_labels \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SCORE \n",
    "def dcg_at_k(r, k, method=1):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k=5, method=1):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def score_predictions(preds, truth, n_modes=5):\n",
    "    \"\"\"\n",
    "    preds: pd.DataFrame\n",
    "      one row for each observation, one column for each prediction.\n",
    "      Columns are sorted from left to right descending in order of likelihood.\n",
    "    truth: pd.Series\n",
    "      one row for each obeservation.\n",
    "    \"\"\"\n",
    "    assert(len(preds)==len(truth))\n",
    "    r = pd.DataFrame(0, index=preds.index, columns=preds.columns, dtype=np.float64)\n",
    "    for col in preds.columns:\n",
    "        r[col] = (preds[col] == truth) * 1.0\n",
    "\n",
    "    score = pd.Series(r.apply(ndcg_at_k, axis=1, reduce=True), name='score')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################  FILE LOAD AND PRE-PROCESS\n",
    "#my_train_plus_session3  contains data for action type and action_detail - only rows with sessions - based on pivot_session.csv\n",
    "#                        scores  0.854761 \\ 0.853916 local and  0.87953 LB      73815 train rows\n",
    "#my_train_plus_session4  same as 3 but conains rows without sessions            213451 train rows  \n",
    "file = ['my_train_plus_session9.csv', 'age_gender_bkts.csv', 'countries.csv',  'my_test_plus_session9.csv'] #'sessions.csv',\n",
    "data = {}\n",
    "for f in file:\n",
    "    data[f.replace('.csv','')]=pd.read_csv('C:\\\\git\\\\Airbnb\\\\data\\\\'+f)\n",
    "\n",
    "#Get train and test data##########################################################################\n",
    "train = data['my_train_plus_session9']\n",
    "test = data['my_test_plus_session9']\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "\n",
    "#train = train[train['country_destination'] != 'NDF']\n",
    "#train = train[train['country_destination'] != 'US']\n",
    "#there are 6065 rows on the train data with first_affiliate_tracked null and only 20 on the test data\n",
    "#train = train[train['first_affiliate_tracked'] != -1]\n",
    "\n",
    "\n",
    "#Remove ids and prediction field and keep them separate##########################################\n",
    "train_ids = train['id']\n",
    "train = train.drop(['id'],axis=1)\n",
    "\n",
    "target = train['country_destination']\n",
    "#train = train.drop(['country_destination'],axis=1)\n",
    "test['country_destination'] = '-'\n",
    "\n",
    "test_ids = test['id']\n",
    "test = test.drop(['id'],axis=1)\n",
    "#np.unique(train[['age']].values)\n",
    "\n",
    "\n",
    "#This is the Model2 Evaluation target data frame\n",
    "dfpivottarget = pd.DataFrame(target)\n",
    "dfpivottarget['AU'] = (dfpivottarget['country_destination'] =='AU')\n",
    "dfpivottarget['CA'] = (dfpivottarget['country_destination'] =='CA')\n",
    "dfpivottarget['DE'] = (dfpivottarget['country_destination'] =='DE')\n",
    "dfpivottarget['ES'] = (dfpivottarget['country_destination'] =='ES')\n",
    "dfpivottarget['FR'] = (dfpivottarget['country_destination'] =='FR')\n",
    "dfpivottarget['GB'] = (dfpivottarget['country_destination'] =='GB')\n",
    "dfpivottarget['IT'] = (dfpivottarget['country_destination'] =='IT')\n",
    "dfpivottarget['NDF'] = (dfpivottarget['country_destination'] =='NDF') #initially commented for M2, but needed for 2F\n",
    "dfpivottarget['NL'] = (dfpivottarget['country_destination'] =='NL')\n",
    "dfpivottarget['other'] = (dfpivottarget['country_destination'] =='other')\n",
    "dfpivottarget['PT'] = (dfpivottarget['country_destination'] =='PT')\n",
    "dfpivottarget['US'] = (dfpivottarget['country_destination'] =='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to_be_dropped = ['about_us',\t'account',\t'acculynk_bin_check_failed',\t'acculynk_bin_check_success',\t'acculynk_load_pin_pad',\t'acculynk_pin_pad_error',\t'acculynk_pin_pad_inactive',\t'acculynk_pin_pad_success',\t'acculynk_session_obtained',\t'active',\t'add_business_address_colorbox',\t'add_guest_colorbox',\t'add_guests',\t'add_note',\t'agree_terms_check',\t'agree_terms_uncheck',\t'airbnb_picks',\t'airbrb',\t'ajax_check_dates',\t'ajax_get_referrals_amt',\t'ajax_get_results',\t'ajax_google_translate',\t'ajax_google_translate_description',\t'ajax_google_translate_reviews',\t'ajax_image_upload',\t'ajax_ldp',\t'ajax_lwlb_contact',\t'ajax_payout_edit',\t'ajax_payout_options_by_country',\t'ajax_payout_split_edit',\t'ajax_photo_widget',\t'ajax_photo_widget_form_iframe',\t'ajax_price_and_availability',\t'ajax_referral_banner_experiment_type',\t'ajax_referral_banner_type',\t'ajax_refresh_subtotal',\t'ajax_send_message',\t'ajax_special_offer_dates_available',\t'ajax_statsd',\t'ajax_worth',\t'apply',\t'apply_code',\t'apply_coupon_click',\t'apply_coupon_click_success',\t'apply_coupon_error',\t'apply_coupon_error_type',\t'apply_reservation',\t'approve',\t'ask_question',\t'at_checkpoint',\t'authenticate',\t'authorize',\t'available',\t'badge',\t'become_user',\t'book',\t'booking',\t'braintree_client_token',\t'business_travel',\t'calendar_tab_inner2',\t'callback',\t'campaigns',\t'cancel',\t'cancellation_policies',\t'cancellation_policy_click',\t'change',\t'change_availability',\t'change_currency',\t'change_default_payout',\t'change_password',\t'check',\t'city_count',\t'clear_reservation',\t'click',\t'clickthrough',\t'collections',\t'complete',\t'complete_redirect',\t'complete_status',\t'concierge',\t'confirm_email',\t'confirmation',\t'connect',\t'contact_new',\t'countries',\t'country_options',\t'coupon_code_click',\t'coupon_field_focus',\t'create',\t'create_ach',\t'create_airbnb',\t'create_multiple',\t'create_paypal',\t'currencies',\t'custom_recommended_destinations',\t'dashboard',\t'deactivate',\t'deactivated',\t'deauthorize',\t'decision_tree',\t'delete',\t'department',\t'departments',\t'desks',\t'destroy',\t'detect_fb_session',\t'disaster_action',\t'domains',\t'edit',\t'edit_verification',\t'email_by_key',\t'email_itinerary_colorbox',\t'email_share',\t'email_wishlist',\t'endpoint_error',\t'envoy_bank_details_redirect',\t'envoy_form',\t'events',\t'facebook_auto_login',\t'faq',\t'faq_category',\t'faq_experiment_ids',\t'feed',\t'forgot_password',\t'founders',\t'friend_listing',\t'friends',\t'friends_new',\t'glob',\t'google_importer',\t'guarantee',\t'guest_billing_receipt',\t'handle_vanity_url',\t'hard_fallback_submit',\t'has_profile_pic',\t'header_userpic',\t'home_safety_landing',\t'home_safety_terms',\t'hospitality',\t'hospitality_standards',\t'host_2013',\t'host_cancel',\t'host_summary',\t'hosting_social_proof',\t'how_it_works',\t'identity',\t'image_order',\t'impressions',\t'index',\t'invalid_action',\t'issue',\t'itinerary',\t'jumio',\t'jumio_redirect',\t'jumio_token',\t'kba',\t'kba_update',\t'languages_multiselect',\t'life',\t'listing',\t'listings',\t'load_more',\t'locale_from_host',\t'localization_settings',\t'localized',\t'locations',\t'login',\t'login_modal',\t'lookup',\t'manage_listing',\t'media_resources',\t'message',\t'mobile_landing_page',\t'mobile_oauth_callback',\t'multi',\t'multi_message_attributes',\t'my',\t'my_listings',\t'my_reservations',\t'new',\t'new_host',\t'new_session',\t'notifications',\t'nyan',\t'office_location',\t'onenight',\t'open_graph_setting',\t'open_hard_fallback_modal',\t'other_hosting_reviews',\t'other_hosting_reviews_first',\t'overview',\t'p4_refund_policy_terms',\t'p4_terms',\t'patch',\t'pay',\t'payment_instruments',\t'payment_methods',\t'payoneer_account_redirect',\t'payoneer_signup_complete',\t'payout_delete',\t'payout_preferences',\t'payout_update',\t'pending',\t'pending_tickets',\t'personalize',\t'phone_number_widget',\t'phone_verification',\t'phone_verification_call_taking_too_long',\t'phone_verification_error',\t'phone_verification_modal',\t'phone_verification_number_submitted_for_call',\t'phone_verification_number_submitted_for_sms',\t'phone_verification_number_sucessfully_submitted',\t'phone_verification_phone_number_removed',\t'phone_verification_success',\t'photography',\t'photography_update',\t'place_worth',\t'plaxo_cb',\t'popular',\t'popular_listing',\t'populate_from_facebook',\t'populate_help_dropdown',\t'position',\t'press_content',\t'press_news',\t'press_release',\t'pricing',\t'print_confirmation',\t'privacy',\t'profile_pic',\t'push_notification_callback',\t'qt2',\t'qt_reply_v2',\t'qt_with',\t'questions',\t'rate',\t'reactivate',\t'read_policy_click',\t'receipt',\t'recent_reservations',\t'recommend',\t'recommendation_page',\t'recommendations',\t'recommended_listings',\t'redirect',\t'references',\t'referrer_status',\t'refund_guest_cancellation',\t'relationship',\t'remove_dashboard_alert',\t'rentals',\t'report',\t'reputation',\t'request_new_confirm_email',\t'request_photography',\t'requested',\t'requirements',\t'reservation',\t'reset_calendar',\t'respond',\t'rest-of-world',\t'revert_to_admin',\t'review_page',\t'reviews',\t'reviews_new',\t'salute',\t'sandy',\t'satisfy',\t'search',\t'search_results',\t'set_default',\t'set_minimum_payout_amount',\t'set_password',\t'set_user',\t'settings',\t'show',\t'show_code',\t'show_personalize',\t'signature',\t'signed_out_modal',\t'signup_login',\t'signup_modal',\t'signup_weibo',\t'signup_weibo_referral',\t'similar_listings',\t'similar_listings_v2',\t'sldf',\t'slideshow',\t'social',\t'social-media',\t'social_connections',\t'south-america',\t'southern-europe',\t'spoken_languages',\t'status',\t'stpcv',\t'sublets',\t'submit_contact',\t'support_phone_numbers',\t'supported',\t'sync',\t'tell_a_friend',\t'terms',\t'terms_and_conditions',\t'this_hosting_reviews',\t'this_hosting_reviews_3000',\t'toggle_archived_thread',\t'toggle_availability',\t'toggle_starred_thread',\t'top_destinations',\t'tos_2014',\t'tos_confirm',\t'track_activity',\t'track_page_view',\t'transaction_history',\t'transaction_history_paginated',\t'travel',\t'travel_plans_current',\t'travel_plans_previous',\t'trust',\t'unavailabilities',\t'united-states',\t'unread',\t'unsubscribe',\t'update',\t'update_cached',\t'update_country_of_residence',\t'update_friends_display',\t'update_hide_from_search_engines',\t'update_message',\t'update_notifications',\t'update_reservation_requirements',\t'upload',\t'uptodate',\t'use_mobile_site',\t'verify',\t'view',\t'views',\t'views_campaign',\t'views_campaign_rules',\t'webcam_upload',\t'weibo_signup_referral_finish',\t'why_host',\t'widget',\t'wishlists',\t'zendesk_login_jwt',\t'-unknown-',\t'Android App Unknown Phone Tablet',\t'Android Phone',\t'Blackberry',\t'Chromebook',\t'Linux Desktop',\t'Mac Desktop',\t'Opera Phone',\t'Tablet',\t'Windows Desktop',\t'Windows Phone',\t'iPad Tablet',\t'iPhone',\t'iPodtouch',\t'secs_elapsed']\n",
    "# train = train.drop(to_be_dropped,axis=1)\n",
    "# test = test.drop(to_be_dropped,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################       APPLY LOGIC ON TRAIN AND TESTS \n",
    "len_train_before = len(train)\n",
    "len_test_before = len(test)\n",
    "\n",
    "trainlen = train.shape[0]\n",
    "dfall = pd.concat((train, test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfall = dfall.drop(['fa_year','ac_year','ac_day','ac_month','fa_day','fa_month'],axis=1) \n",
    "dfall = dfall.drop(['accept_decline'],axis=1) #all values are zeros on train, only one ID on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NUMBER OF MESSAGES\n",
    "dfall['num_messages'] = dfall['empty'] +  dfall['10'] + dfall['11'] + dfall['12'] + dfall['15'] + dfall['maybe_information'] + dfall['multi_message'] + dfall['preapproval'] + dfall['special_offer'] + dfall['message_to_host_focus'] + dfall['message_to_host_change'] #+ dfall['ajax_send_message'] + dfall['multi_message_attributes'] + dfall['message'] + dfall['update_message']\n",
    "dfall = dfall.drop(['empty','10','11','12', '15','maybe_information','multi_message','preapproval','special_offer','message_to_host_focus', 'message_to_host_change'],axis=1) #'ajax_send_message','multi_message_attributes','message','update_message'\n",
    "\n",
    "dfall.loc[(dfall.num_messages <= 0), 'num_messages'] = 'NoMessages' \n",
    "dfall.loc[(dfall.num_messages >= 69), 'num_messages'] = '69+Messages' #train only goes til 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set whoever did more than 1 bookng requestes to 1\n",
    "dfall.loc[dfall.num_AT_booking_request >=1, 'num_AT_booking_request'] = 1\n",
    "dfall.loc[dfall.guest_booked_elsewhere >=1, 'guest_booked_elsewhere'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AGE MANIPULATION\n",
    "#rememeber that I set everything <14 and >100 as -1 of the way in\n",
    "dfall.loc[(dfall.age <=14), 'age_bucket'] = '-1' # 0-4 / 5-9 / 10-14\n",
    "dfall.loc[(dfall.age >=15) & (dfall.age <= 19), 'age_bucket'] = '15-19'\n",
    "dfall.loc[(dfall.age >=20) & (dfall.age <= 24), 'age_bucket'] = '20-24'\n",
    "dfall.loc[(dfall.age >=25) & (dfall.age <= 29), 'age_bucket'] = '25-29'\n",
    "dfall.loc[(dfall.age >=30) & (dfall.age <= 34), 'age_bucket'] = '30-34'\n",
    "dfall.loc[(dfall.age >=35) & (dfall.age <= 39), 'age_bucket'] = '35-39'\n",
    "dfall.loc[(dfall.age >=40) & (dfall.age <= 44), 'age_bucket'] = '40-44'\n",
    "dfall.loc[(dfall.age >=45) & (dfall.age <= 49), 'age_bucket'] = '45-49'\n",
    "dfall.loc[(dfall.age >=50) & (dfall.age <= 54), 'age_bucket'] = '50-54'\n",
    "dfall.loc[(dfall.age >=55) & (dfall.age <= 59), 'age_bucket'] = '55-59'\n",
    "dfall.loc[(dfall.age >=60) & (dfall.age <= 64), 'age_bucket'] = '60-64'\n",
    "dfall.loc[(dfall.age >=65) & (dfall.age <= 69), 'age_bucket'] = '65-69'\n",
    "dfall.loc[(dfall.age >=70) & (dfall.age <= 74), 'age_bucket'] = '70-74'\n",
    "dfall.loc[(dfall.age >=75) & (dfall.age <= 79), 'age_bucket'] = '75-79'\n",
    "dfall.loc[(dfall.age >=80) & (dfall.age <= 84), 'age_bucket'] = '80-84'\n",
    "dfall.loc[(dfall.age >=85) & (dfall.age <= 89), 'age_bucket'] = '85-89'\n",
    "dfall.loc[(dfall.age >=90) & (dfall.age <= 94), 'age_bucket'] = '90-94'\n",
    "dfall.loc[(dfall.age >=95) & (dfall.age <= 99), 'age_bucket'] = '95-99'\n",
    "dfall.loc[(dfall.age >=100), 'age_bucket'] = '100+'\n",
    "dfall = dfall.drop(['age'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ACTIONS - no improvement\n",
    "# l=['actionB1','actionB2','actionB3','actionB4','actionB5','actionB6','actionB7','actionB8','actionB9','actionB10','actionB11','actionB12','actionB13','actionB14','actionB15']\n",
    "# actionBuckets = pd.cut(dfall['num_actions'], 15, retbins=True, labels = l)\n",
    "# print 'Number of Actions Buckets:', actionBuckets[1]\n",
    "# dfall['num_actions_bucket'] = actionBuckets[0]\n",
    "# dfall = dfall.drop(['num_actions'],axis=1)\n",
    "\n",
    "# dfall.loc[(dfall.num_actions <=82), 'action_bucket'] = 'lessthan82'\n",
    "# dfall.loc[(dfall.num_actions >=83) & (dfall.num_actions <= 164), 'action_bucket'] = '83-164'\n",
    "# dfall.loc[(dfall.num_actions >=165) & (dfall.num_actions <= 246), 'action_bucket'] = '165-246'\n",
    "# dfall.loc[(dfall.num_actions >=247) & (dfall.num_actions <= 328), 'action_bucket'] = '247-328'\n",
    "# dfall.loc[(dfall.num_actions >=329) & (dfall.num_actions <= 410), 'action_bucket'] = '329-410'\n",
    "# dfall.loc[(dfall.num_actions >=411) & (dfall.num_actions <= 492), 'action_bucket'] = '411-492'\n",
    "# dfall.loc[(dfall.num_actions >=493) & (dfall.num_actions <= 574), 'action_bucket'] = '493-574'\n",
    "# dfall.loc[(dfall.num_actions >=575) & (dfall.num_actions <= 656), 'action_bucket'] = '575-656'\n",
    "# dfall.loc[(dfall.num_actions >=657) & (dfall.num_actions <= 738), 'action_bucket'] = '657-738'\n",
    "# dfall.loc[(dfall.num_actions >=739) & (dfall.num_actions <= 820), 'action_bucket'] = '739-820'\n",
    "# dfall.loc[(dfall.num_actions >=821) & (dfall.num_actions <= 902), 'action_bucket'] = '821-902'\n",
    "# dfall.loc[(dfall.num_actions >=903) & (dfall.num_actions <= 984), 'action_bucket'] = '903-984'\n",
    "# dfall.loc[(dfall.num_actions >=985) & (dfall.num_actions <= 1066), 'action_bucket'] = '985-1066'\n",
    "# dfall.loc[(dfall.num_actions >=1067), 'action_bucket'] = 'morethan1067'\n",
    "# dfall = dfall.drop(['num_actions'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding:  gender\n",
      "Encoding:  signup_method\n",
      "Encoding:  language\n",
      "Encoding:  affiliate_channel\n",
      "Encoding:  affiliate_provider\n",
      "Encoding:  first_affiliate_tracked\n",
      "Encoding:  signup_app\n",
      "Encoding:  first_device_type\n",
      "Encoding:  first_browser\n"
     ]
    }
   ],
   "source": [
    "cols = ['age_bucket', 'gender', 'signup_method','signup_flow','language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser']\n",
    "   \n",
    "dfall2 = dfall[cols].copy()\n",
    "\n",
    "class MyEncoder:\n",
    "    labelencoder={}\n",
    "\n",
    "    def encode(self, d, col, labelencoder={}):\n",
    "        if col not in labelencoder:\n",
    "            labelencoder[col] = {}\n",
    "        result = []\n",
    "        for i in d:\n",
    "            if i not in labelencoder[col]:\n",
    "                labelencoder[col][i] = len(labelencoder[col])+1\n",
    "                result.append(labelencoder[col][i])\n",
    "            else:\n",
    "                result.append(labelencoder[col][i])\n",
    "        return result,labelencoder\n",
    "\n",
    "encodings={}\n",
    "encoder = MyEncoder()\n",
    "\n",
    "for col in dfall2.columns:\n",
    "    if dfall2[col].dtypes == object and col != 'age_bucket':\n",
    "        print 'Encoding: ',col\n",
    "        dfall2[col], encodings = encoder.encode(dfall2[col].values,col,encodings)    \n",
    "\n",
    "cols.remove('age_bucket')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#best features\n",
    "# import operator\n",
    "# k_best_features = SelectKBestFeatures(dfall2.ix[:, dfall2.columns != 'age_bucket'],dfall2['age_bucket'], 250, False)\n",
    "# sorted_x  = sorted(k_best_features.items(), key=operator.itemgetter(1))\n",
    "# (sorted_x).reverse()\n",
    "\n",
    "# #print sorted_x\n",
    "# for a,b in sorted_x:\n",
    "#      print a,b\n",
    "\n",
    "# KNN GridSearchCV on the age model\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# def suggestKNeighborsClassifierModel(train, target):\n",
    "    \n",
    "#     neighbor_params = [{'n_neighbors' : range(2,6)\n",
    "#                            #, 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "#                            , 'weights': ['uniform', 'distance']\n",
    "#                            , 'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "#                        }]\n",
    "#     clf = GridSearchCV(KNeighborsClassifier(), neighbor_params) # , scoring='f1'\n",
    "#     best = clf.fit(train, target)\n",
    "#     print best.best_estimator_\n",
    "    \n",
    "# suggestKNeighborsClassifierModel(dfall2.loc[(dfall2.age_bucket != '-1')] [cols], \n",
    "#                                  dfall2.loc[(dfall2.age_bucket != '-1')]['age_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols =['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider',\n",
    "  'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "#cols =['gender', 'signup_method','signup_flow', 'signup_app', 'first_device_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16975\n",
      "0.230272528725\n"
     ]
    }
   ],
   "source": [
    "# agemodel = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
    "#            metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
    "# agemodel.fit (dfall2.loc[(dfall2.age_bucket != '-1')] [cols], dfall2.loc[(dfall2.age_bucket != '-1')] ['age_bucket'])\n",
    "\n",
    "# preds = agemodel.predict(dfall2[cols])\n",
    "# dfall2['pred_age_bucket'] = preds\n",
    "# print len(dfall2.loc[(dfall2.age_bucket != '-1') & (dfall2.age_bucket == dfall2.pred_age_bucket)] )\n",
    "# print len(dfall2.loc[(dfall2.age_bucket != '-1') & (dfall2.age_bucket == dfall2.pred_age_bucket)] ) / float(len(dfall2.loc[(dfall2.age_bucket != '-1')]) )\n",
    "\n",
    "# 16975\n",
    "# 0.230272528725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21276\n",
      "0.288617279596\n"
     ]
    }
   ],
   "source": [
    "params = {'eta': 0.1,\n",
    "          'max_depth': 10,\n",
    "          'subsample': 0.5,\n",
    "          'colsample_bytree': 0.5,\n",
    "          'objective': 'multi:softmax',\n",
    "          'num_class': 18}\n",
    "\n",
    "le = LabelEncoder()\n",
    "X= dfall2.loc[(dfall2.age_bucket != '-1')] [cols].values\n",
    "y = le.fit_transform(dfall2.loc[(dfall2.age_bucket != '-1')] ['age_bucket'])   \n",
    "\n",
    "xgbmodel = xgb.train(params=params, dtrain=xgb.DMatrix(X, y), num_boost_round=100)                    \n",
    "pred = xgbmodel.predict(xgb.DMatrix(dfall2[cols].values)) \n",
    "pred = np.array(pred, dtype=np.int32)\n",
    "\n",
    "dfall2['pred_age_bucket'] = le.inverse_transform(pred)\n",
    "print len(dfall2.loc[(dfall2.age_bucket != '-1') & (dfall2.age_bucket == dfall2.pred_age_bucket)] )\n",
    "print len(dfall2.loc[(dfall2.age_bucket != '-1') & (dfall2.age_bucket == dfall2.pred_age_bucket)] ) / float(len(dfall2.loc[(dfall2.age_bucket != '-1')]) )\n",
    "\n",
    "# 21276\n",
    "# 0.288617279596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfall['pred_age_bucket'] = le.inverse_transform(pred)\n",
    "dfall.loc[(dfall.age_bucket == '-1'), 'age_bucket'] = dfall.pred_age_bucket\n",
    "dfall = dfall.drop(['pred_age_bucket'],axis=1)\n",
    "#dfall[['age_bucket', 'pred_age_bucket']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Population in K by age bucket based on users age bucket\n",
    "popInKByAgeBucket=pd.read_csv('C:\\\\git\\\\Airbnb\\\\aux_data\\\\popInKByAgeBucket.csv')\n",
    "dfall = pd.merge(dfall, popInKByAgeBucket, on='age_bucket', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfall['translate'] = dfall['ajax_google_translate_reviews'] + dfall['ajax_google_translate_description'] + dfall['ajax_google_translate']\n",
    "#dfall = dfall.drop(['ajax_google_translate_reviews', 'ajax_google_translate_description', 'ajax_google_translate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummying:  gender\n",
      "Dummying:  signup_method\n",
      "Dummying:  signup_flow\n",
      "Dummying:  language\n",
      "Dummying:  affiliate_channel\n",
      "Dummying:  affiliate_provider\n",
      "Dummying:  first_affiliate_tracked\n",
      "Dummying:  signup_app\n",
      "Dummying:  first_device_type\n",
      "Dummying:  first_browser\n",
      "Dummying:  num_AT_booking_request\n",
      "Dummying:  num_AD_pending\n",
      "Not touching:  num_actions\n",
      "Not touching:  start_from_zero\n",
      "Not touching:  about_us\n",
      "Not touching:  account\n",
      "Not touching:  acculynk_bin_check_failed\n",
      "Not touching:  acculynk_bin_check_success\n",
      "Not touching:  acculynk_load_pin_pad\n",
      "Not touching:  acculynk_pin_pad_error\n",
      "Not touching:  acculynk_pin_pad_inactive\n",
      "Not touching:  acculynk_pin_pad_success\n",
      "Not touching:  acculynk_session_obtained\n",
      "Not touching:  active\n",
      "Not touching:  add_business_address_colorbox\n",
      "Not touching:  add_guest_colorbox\n",
      "Not touching:  add_guests\n",
      "Not touching:  add_note\n",
      "Not touching:  agree_terms_check\n",
      "Not touching:  agree_terms_uncheck\n",
      "Not touching:  airbnb_picks\n",
      "Not touching:  airbrb\n",
      "Not touching:  ajax_check_dates\n",
      "Not touching:  ajax_get_referrals_amt\n",
      "Not touching:  ajax_get_results\n",
      "Dummying:  ajax_google_translate\n",
      "Dummying:  ajax_google_translate_description\n",
      "Dummying:  ajax_google_translate_reviews\n",
      "Not touching:  ajax_image_upload\n",
      "Not touching:  ajax_ldp\n",
      "Not touching:  ajax_lwlb_contact\n",
      "Not touching:  ajax_payout_edit\n",
      "Not touching:  ajax_payout_options_by_country\n",
      "Not touching:  ajax_payout_split_edit\n",
      "Not touching:  ajax_photo_widget\n",
      "Not touching:  ajax_photo_widget_form_iframe\n",
      "Not touching:  ajax_price_and_availability\n",
      "Not touching:  ajax_referral_banner_experiment_type\n",
      "Not touching:  ajax_referral_banner_type\n",
      "Not touching:  ajax_refresh_subtotal\n",
      "Not touching:  ajax_send_message\n",
      "Not touching:  ajax_special_offer_dates_available\n",
      "Not touching:  ajax_statsd\n",
      "Not touching:  ajax_worth\n",
      "Not touching:  apply\n",
      "Not touching:  apply_code\n",
      "Not touching:  apply_coupon_click\n",
      "Not touching:  apply_coupon_click_success\n",
      "Not touching:  apply_coupon_error\n",
      "Not touching:  apply_coupon_error_type\n",
      "Not touching:  apply_reservation\n",
      "Not touching:  approve\n",
      "Not touching:  ask_question\n",
      "Not touching:  at_checkpoint\n",
      "Not touching:  authenticate\n",
      "Not touching:  authorize\n",
      "Not touching:  available\n",
      "Not touching:  badge\n",
      "Not touching:  become_user\n",
      "Not touching:  book\n",
      "Not touching:  booking\n",
      "Not touching:  braintree_client_token\n",
      "Not touching:  business_travel\n",
      "Not touching:  calendar_tab_inner2\n",
      "Not touching:  callback\n",
      "Not touching:  campaigns\n",
      "Not touching:  cancel\n",
      "Not touching:  cancellation_policies\n",
      "Not touching:  cancellation_policy_click\n",
      "Not touching:  change\n",
      "Not touching:  change_availability\n",
      "Not touching:  change_currency\n",
      "Not touching:  change_default_payout\n",
      "Not touching:  change_password\n",
      "Not touching:  check\n",
      "Not touching:  city_count\n",
      "Not touching:  clear_reservation\n",
      "Not touching:  click\n",
      "Not touching:  clickthrough\n",
      "Not touching:  collections\n",
      "Not touching:  complete\n",
      "Not touching:  complete_redirect\n",
      "Not touching:  complete_status\n",
      "Not touching:  concierge\n",
      "Not touching:  confirm_email\n",
      "Not touching:  confirmation\n",
      "Not touching:  connect\n",
      "Not touching:  contact_new\n",
      "Not touching:  countries\n",
      "Not touching:  country_options\n",
      "Not touching:  coupon_code_click\n",
      "Not touching:  coupon_field_focus\n",
      "Not touching:  create\n",
      "Not touching:  create_ach\n",
      "Not touching:  create_airbnb\n",
      "Not touching:  create_multiple\n",
      "Not touching:  create_paypal\n",
      "Not touching:  currencies\n",
      "Not touching:  custom_recommended_destinations\n",
      "Not touching:  dashboard\n",
      "Not touching:  deactivate\n",
      "Not touching:  deactivated\n",
      "Not touching:  deauthorize\n",
      "Not touching:  decision_tree\n",
      "Not touching:  delete\n",
      "Not touching:  department\n",
      "Not touching:  departments\n",
      "Not touching:  desks\n",
      "Not touching:  destroy\n",
      "Not touching:  detect_fb_session\n",
      "Not touching:  disaster_action\n",
      "Not touching:  domains\n",
      "Not touching:  edit\n",
      "Not touching:  edit_verification\n",
      "Not touching:  email_by_key\n",
      "Not touching:  email_itinerary_colorbox\n",
      "Not touching:  email_share\n",
      "Not touching:  email_wishlist\n",
      "Not touching:  endpoint_error\n",
      "Not touching:  envoy_bank_details_redirect\n",
      "Not touching:  envoy_form\n",
      "Not touching:  events\n",
      "Not touching:  facebook_auto_login\n",
      "Not touching:  faq\n",
      "Not touching:  faq_category\n",
      "Not touching:  faq_experiment_ids\n",
      "Not touching:  feed\n",
      "Not touching:  forgot_password\n",
      "Not touching:  founders\n",
      "Not touching:  friend_listing\n",
      "Not touching:  friends\n",
      "Not touching:  friends_new\n",
      "Not touching:  glob\n",
      "Not touching:  google_importer\n",
      "Not touching:  guarantee\n",
      "Not touching:  guest_billing_receipt\n",
      "Not touching:  guest_booked_elsewhere\n",
      "Not touching:  handle_vanity_url\n",
      "Not touching:  hard_fallback_submit\n",
      "Not touching:  has_profile_pic\n",
      "Not touching:  header_userpic\n",
      "Not touching:  home_safety_landing\n",
      "Not touching:  home_safety_terms\n",
      "Not touching:  hospitality\n",
      "Not touching:  hospitality_standards\n",
      "Not touching:  host_2013\n",
      "Not touching:  host_cancel\n",
      "Not touching:  host_summary\n",
      "Not touching:  hosting_social_proof\n",
      "Not touching:  how_it_works\n",
      "Not touching:  identity\n",
      "Not touching:  image_order\n",
      "Not touching:  impressions\n",
      "Not touching:  index\n",
      "Not touching:  invalid_action\n",
      "Not touching:  issue\n",
      "Not touching:  itinerary\n",
      "Not touching:  jumio\n",
      "Not touching:  jumio_redirect\n",
      "Not touching:  jumio_token\n",
      "Not touching:  kba\n",
      "Not touching:  kba_update\n",
      "Not touching:  languages_multiselect\n",
      "Not touching:  life\n",
      "Not touching:  listing\n",
      "Not touching:  listings\n",
      "Not touching:  load_more\n",
      "Not touching:  locale_from_host\n",
      "Not touching:  localization_settings\n",
      "Not touching:  localized\n",
      "Not touching:  locations\n",
      "Not touching:  login\n",
      "Not touching:  login_modal\n",
      "Not touching:  lookup\n",
      "Not touching:  manage_listing\n",
      "Not touching:  media_resources\n",
      "Not touching:  message\n",
      "Not touching:  mobile_landing_page\n",
      "Not touching:  mobile_oauth_callback\n",
      "Not touching:  multi\n",
      "Not touching:  multi_message_attributes\n",
      "Not touching:  my\n",
      "Not touching:  my_listings\n",
      "Not touching:  my_reservations\n",
      "Not touching:  new\n",
      "Not touching:  new_host\n",
      "Not touching:  new_session\n",
      "Not touching:  notifications\n",
      "Not touching:  nyan\n",
      "Not touching:  office_location\n",
      "Not touching:  onenight\n",
      "Not touching:  open_graph_setting\n",
      "Not touching:  open_hard_fallback_modal\n",
      "Not touching:  other_hosting_reviews\n",
      "Not touching:  other_hosting_reviews_first\n",
      "Not touching:  overview\n",
      "Not touching:  p4_refund_policy_terms\n",
      "Not touching:  p4_terms\n",
      "Not touching:  patch\n",
      "Not touching:  pay\n",
      "Not touching:  payment_instruments\n",
      "Not touching:  payment_methods\n",
      "Not touching:  payoneer_account_redirect\n",
      "Not touching:  payoneer_signup_complete\n",
      "Not touching:  payout_delete\n",
      "Not touching:  payout_preferences\n",
      "Not touching:  payout_update\n",
      "Not touching:  pending\n",
      "Not touching:  pending_tickets\n",
      "Not touching:  personalize\n",
      "Not touching:  phone_number_widget\n",
      "Not touching:  phone_verification\n",
      "Not touching:  phone_verification_call_taking_too_long\n",
      "Not touching:  phone_verification_error\n",
      "Not touching:  phone_verification_modal\n",
      "Not touching:  phone_verification_number_submitted_for_call\n",
      "Not touching:  phone_verification_number_submitted_for_sms\n",
      "Not touching:  phone_verification_number_sucessfully_submitted\n",
      "Not touching:  phone_verification_phone_number_removed\n",
      "Not touching:  phone_verification_success\n",
      "Not touching:  photography\n",
      "Not touching:  photography_update\n",
      "Not touching:  place_worth\n",
      "Not touching:  plaxo_cb\n",
      "Not touching:  popular\n",
      "Not touching:  popular_listing\n",
      "Not touching:  populate_from_facebook\n",
      "Not touching:  populate_help_dropdown\n",
      "Not touching:  position\n",
      "Not touching:  press_content\n",
      "Not touching:  press_news\n",
      "Not touching:  press_release\n",
      "Not touching:  pricing\n",
      "Not touching:  print_confirmation\n",
      "Not touching:  privacy\n",
      "Not touching:  profile_pic\n",
      "Not touching:  push_notification_callback\n",
      "Not touching:  qt2\n",
      "Not touching:  qt_reply_v2\n",
      "Not touching:  qt_with\n",
      "Not touching:  questions\n",
      "Not touching:  rate\n",
      "Not touching:  reactivate\n",
      "Not touching:  read_policy_click\n",
      "Not touching:  receipt\n",
      "Not touching:  recent_reservations\n",
      "Not touching:  recommend\n",
      "Not touching:  recommendation_page\n",
      "Not touching:  recommendations\n",
      "Not touching:  recommended_listings\n",
      "Not touching:  redirect\n",
      "Not touching:  references\n",
      "Not touching:  referrer_status\n",
      "Not touching:  refund_guest_cancellation\n",
      "Not touching:  relationship\n",
      "Not touching:  remove_dashboard_alert\n",
      "Not touching:  rentals\n",
      "Not touching:  report\n",
      "Not touching:  reputation\n",
      "Not touching:  request_new_confirm_email\n",
      "Not touching:  request_photography\n",
      "Not touching:  requested\n",
      "Not touching:  requirements\n",
      "Not touching:  reservation\n",
      "Not touching:  reset_calendar\n",
      "Not touching:  respond\n",
      "Not touching:  rest-of-world\n",
      "Not touching:  revert_to_admin\n",
      "Not touching:  review_page\n",
      "Not touching:  reviews\n",
      "Not touching:  reviews_new\n",
      "Not touching:  salute\n",
      "Not touching:  sandy\n",
      "Not touching:  satisfy\n",
      "Not touching:  search\n",
      "Not touching:  search_results\n",
      "Not touching:  set_default\n",
      "Not touching:  set_minimum_payout_amount\n",
      "Not touching:  set_password\n",
      "Not touching:  set_user\n",
      "Not touching:  settings\n",
      "Not touching:  show\n",
      "Not touching:  show_code\n",
      "Not touching:  show_personalize\n",
      "Not touching:  signature\n",
      "Not touching:  signed_out_modal\n",
      "Not touching:  signup_login\n",
      "Not touching:  signup_modal\n",
      "Not touching:  signup_weibo\n",
      "Not touching:  signup_weibo_referral\n",
      "Not touching:  similar_listings\n",
      "Not touching:  similar_listings_v2\n",
      "Not touching:  sldf\n",
      "Not touching:  slideshow\n",
      "Not touching:  social\n",
      "Not touching:  social-media\n",
      "Not touching:  social_connections\n",
      "Not touching:  south-america\n",
      "Not touching:  southern-europe\n",
      "Not touching:  spoken_languages\n",
      "Not touching:  status\n",
      "Not touching:  stpcv\n",
      "Not touching:  sublets\n",
      "Not touching:  submit_contact\n",
      "Not touching:  support_phone_numbers\n",
      "Not touching:  supported\n",
      "Not touching:  sync\n",
      "Not touching:  tell_a_friend\n",
      "Not touching:  terms\n",
      "Not touching:  terms_and_conditions\n",
      "Not touching:  this_hosting_reviews\n",
      "Not touching:  this_hosting_reviews_3000\n",
      "Not touching:  toggle_archived_thread\n",
      "Not touching:  toggle_availability\n",
      "Not touching:  toggle_starred_thread\n",
      "Not touching:  top_destinations\n",
      "Not touching:  tos_2014\n",
      "Not touching:  tos_confirm\n",
      "Not touching:  track_activity\n",
      "Not touching:  track_page_view\n",
      "Not touching:  transaction_history\n",
      "Not touching:  transaction_history_paginated\n",
      "Not touching:  travel\n",
      "Not touching:  travel_plans_current\n",
      "Not touching:  travel_plans_previous\n",
      "Not touching:  trust\n",
      "Not touching:  unavailabilities\n",
      "Not touching:  united-states\n",
      "Not touching:  unread\n",
      "Not touching:  unsubscribe\n",
      "Not touching:  update\n",
      "Not touching:  update_cached\n",
      "Not touching:  update_country_of_residence\n",
      "Not touching:  update_friends_display\n",
      "Not touching:  update_hide_from_search_engines\n",
      "Not touching:  update_message\n",
      "Not touching:  update_notifications\n",
      "Not touching:  update_reservation_requirements\n",
      "Not touching:  upload\n",
      "Not touching:  uptodate\n",
      "Not touching:  use_mobile_site\n",
      "Not touching:  verify\n",
      "Not touching:  view\n",
      "Not touching:  views\n",
      "Not touching:  views_campaign\n",
      "Not touching:  views_campaign_rules\n",
      "Not touching:  webcam_upload\n",
      "Not touching:  weibo_signup_referral_finish\n",
      "Not touching:  why_host\n",
      "Not touching:  widget\n",
      "Not touching:  wishlists\n",
      "Not touching:  zendesk_login_jwt\n",
      "Not touching:  -unknown-\n",
      "Not touching:  Android App Unknown Phone Tablet\n",
      "Not touching:  Android Phone\n",
      "Not touching:  Blackberry\n",
      "Not touching:  Chromebook\n",
      "Not touching:  Linux Desktop\n",
      "Not touching:  Mac Desktop\n",
      "Not touching:  Opera Phone\n",
      "Not touching:  Tablet\n",
      "Not touching:  Windows Desktop\n",
      "Not touching:  Windows Phone\n",
      "Not touching:  iPad Tablet\n",
      "Not touching:  iPhone\n",
      "Not touching:  iPodtouch\n",
      "Not touching:  secs_elapsed\n",
      "Not touching:  country_destination\n",
      "Dummying:  num_messages\n",
      "Dummying:  age_bucket\n",
      "Not touching:  PopInKtoAU\n",
      "Not touching:  PopInKtoCA\n",
      "Not touching:  PopInKtoDE\n",
      "Not touching:  PopInKtoES\n",
      "Not touching:  PopInKtoFR\n",
      "Not touching:  PopInKtoGB\n",
      "Not touching:  PopInKtoIT\n",
      "Not touching:  PopInKtoNL\n",
      "Not touching:  PopInKtoPT\n",
      "Not touching:  PopInKtoUS\n"
     ]
    }
   ],
   "source": [
    "######################       DUMMY THE DATASET's COLUMNS - apply one hot encoding\n",
    "#need to be on the joined dataset to be sure they end up with the same colums\n",
    "extra_one_hot = ['signup_flow',  'num_AT_booking_request', 'num_AD_pending', 'ajax_google_translate_reviews',\n",
    "                 'ajax_google_translate_description', 'ajax_google_translate']\n",
    "                 #, 'requested']\n",
    "\n",
    "for col in dfall.columns:    \n",
    "    if (col != 'country_destination' and (dfall[col].dtypes == object or col in extra_one_hot)):#if (dfall[col].dtypes == object or col in extra_one_hot):  \n",
    "        print 'Dummying: ',col\n",
    "        dfall_dummy = pd.get_dummies(dfall[col], prefix='is'+col)\n",
    "        dfall = dfall.drop([col], axis=1)\n",
    "        dfall = pd.concat((dfall, dfall_dummy), axis=1)\n",
    "    else:\n",
    "        print 'Not touching: ',col\n",
    "\n",
    "train = dfall[:trainlen]\n",
    "test = dfall[trainlen:]\n",
    "\n",
    "test = test.drop(['country_destination'],axis=1)\n",
    "\n",
    "assert(len_train_before == len(train))\n",
    "assert(len_test_before == len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 107 columns:  ['acculynk_bin_check_failed', 'acculynk_pin_pad_error', 'acculynk_pin_pad_success', 'add_business_address_colorbox', 'add_guest_colorbox', 'ajax_payout_split_edit', 'book', 'braintree_client_token', 'business_travel', 'change_default_payout', 'city_count', 'confirmation', 'create_airbnb', 'custom_recommended_destinations', 'deactivate', 'deactivated', 'disaster_action', 'email_by_key', 'envoy_form', 'events', 'hard_fallback_submit', 'host_cancel', 'localized', 'my_reservations', 'nyan', 'payout_delete', 'phone_verification', 'press_content', 'questions', 'reactivate', 'recommendation_page', 'refund_guest_cancellation', 'rentals', 'report', 'reset_calendar', 'rest-of-world', 'revert_to_admin', 'sandy', 'satisfy', 'set_default', 'set_minimum_payout_amount', 'sldf', 'south-america', 'southern-europe', 'stpcv', 'support_phone_numbers', 'sync', 'this_hosting_reviews_3000', 'tos_2014', 'track_activity', 'unsubscribe', 'use_mobile_site', 'view', 'views', 'views_campaign_rules', 'issignup_method_weibo', 'issignup_flow_14', 'issignup_flow_21', 'islanguage_-unknown-', 'islanguage_ca', 'islanguage_id', 'isaffiliate_provider_craigslist', 'isfirst_browser_AvantBrowser', 'isfirst_browser_CometBird', 'isfirst_browser_IBrowse', 'isfirst_browser_IceDragon', 'isfirst_browser_Mozilla', 'isfirst_browser_NintendoBrowser', 'isfirst_browser_OperaMobile', 'isfirst_browser_Outlook2007', 'isfirst_browser_PaleMoon', 'isfirst_browser_UCBrowser', 'isfirst_browser_Yandex.Browser', 'isnum_AD_pending_7', 'isajax_google_translate_-1.0', 'isajax_google_translate_7.0', 'isajax_google_translate_8.0', 'isajax_google_translate_9.0', 'isajax_google_translate_12.0', 'isajax_google_translate_14.0', 'isajax_google_translate_18.0', 'isajax_google_translate_31.0', 'isajax_google_translate_description_-1.0', 'isajax_google_translate_description_10.0', 'isajax_google_translate_description_12.0', 'isajax_google_translate_description_21.0', 'isajax_google_translate_reviews_-1.0', 'isajax_google_translate_reviews_9.0', 'isajax_google_translate_reviews_13.0', 'isajax_google_translate_reviews_19.0', 'isnum_messages_34.0', 'isnum_messages_36.0', 'isnum_messages_39.0', 'isnum_messages_43.0', 'isnum_messages_44.0', 'isnum_messages_46.0', 'isnum_messages_48.0', 'isnum_messages_50.0', 'isnum_messages_52.0', 'isnum_messages_53.0', 'isnum_messages_55.0', 'isnum_messages_56.0', 'isnum_messages_58.0', 'isnum_messages_60.0', 'isnum_messages_61.0', 'isnum_messages_63.0', 'isnum_messages_64.0']\n"
     ]
    }
   ],
   "source": [
    "#After encoding, drop all columns that have unique values outise NDF because they wont add prediction to non-NDF classes\n",
    "#cannot drop on dfall because they were created on the one-hot-encoding\n",
    "to_be_dropped = []\n",
    "for col in train.columns:\n",
    "    if len(np.unique(train[train['country_destination'] != 'NDF'][col])) ==1:\n",
    "    #if len(np.unique(train[col])) == 1:\n",
    "        to_be_dropped.append(col)\n",
    "        \n",
    "train = train.drop(to_be_dropped,axis=1)\n",
    "test = test.drop(to_be_dropped,axis=1)\n",
    "print 'Dropped', len(to_be_dropped), 'columns: ',to_be_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.998361\n",
       "1     0.001152\n",
       "2     0.000312\n",
       "3     0.000122\n",
       "4     0.000027\n",
       "13    0.000014\n",
       "5     0.000014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['about_us'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 331 columns:  ['about_us', 'acculynk_bin_check_success', 'acculynk_load_pin_pad', 'acculynk_pin_pad_inactive', 'acculynk_session_obtained', 'add_guests', 'add_note', 'agree_terms_uncheck', 'airbnb_picks', 'airbrb', 'ajax_get_results', 'ajax_google_translate', 'ajax_google_translate_description', 'ajax_ldp', 'ajax_payout_edit', 'ajax_payout_options_by_country', 'ajax_photo_widget', 'ajax_price_and_availability', 'ajax_send_message', 'ajax_special_offer_dates_available', 'ajax_worth', 'apply', 'apply_code', 'apply_coupon_click', 'apply_coupon_click_success', 'apply_coupon_error', 'apply_coupon_error_type', 'approve', 'authorize', 'badge', 'become_user', 'booking', 'cancel', 'cancellation_policy_click', 'change', 'change_availability', 'change_currency', 'change_password', 'check', 'clear_reservation', 'clickthrough', 'complete', 'concierge', 'contact_new', 'countries', 'country_options', 'coupon_code_click', 'coupon_field_focus', 'create_ach', 'create_multiple', 'create_paypal', 'currencies', 'deauthorize', 'department', 'departments', 'desks', 'destroy', 'detect_fb_session', 'email_itinerary_colorbox', 'email_share', 'email_wishlist', 'endpoint_error', 'envoy_bank_details_redirect', 'feed', 'forgot_password', 'founders', 'friend_listing', 'friends', 'friends_new', 'glob', 'google_importer', 'guarantee', 'guest_billing_receipt', 'has_profile_pic', 'home_safety_landing', 'home_safety_terms', 'hospitality', 'hospitality_standards', 'host_2013', 'host_summary', 'how_it_works', 'image_order', 'invalid_action', 'issue', 'itinerary', 'jumio', 'life', 'listing', 'load_more', 'locale_from_host', 'localization_settings', 'locations', 'media_resources', 'message', 'mobile_landing_page', 'mobile_oauth_callback', 'multi', 'multi_message_attributes', 'my_listings', 'new_host', 'new_session', 'office_location', 'onenight', 'open_hard_fallback_modal', 'other_hosting_reviews', 'overview', 'p4_refund_policy_terms', 'p4_terms', 'patch', 'pay', 'payoneer_account_redirect', 'payoneer_signup_complete', 'payout_update', 'pending_tickets', 'phone_verification_call_taking_too_long', 'phone_verification_error', 'phone_verification_number_submitted_for_call', 'phone_verification_number_submitted_for_sms', 'phone_verification_number_sucessfully_submitted', 'phone_verification_phone_number_removed', 'phone_verification_success', 'photography', 'photography_update', 'place_worth', 'plaxo_cb', 'popular', 'popular_listing', 'position', 'press_news', 'press_release', 'pricing', 'print_confirmation', 'privacy', 'push_notification_callback', 'rate', 'read_policy_click', 'receipt', 'recommend', 'redirect', 'relationship', 'remove_dashboard_alert', 'reputation', 'request_photography', 'requirements', 'reservation', 'respond', 'review_page', 'salute', 'set_password', 'show_code', 'signed_out_modal', 'signup_modal', 'signup_weibo', 'signup_weibo_referral', 'slideshow', 'social', 'social-media', 'spoken_languages', 'status', 'sublets', 'submit_contact', 'supported', 'tell_a_friend', 'terms', 'terms_and_conditions', 'toggle_archived_thread', 'toggle_availability', 'toggle_starred_thread', 'top_destinations', 'transaction_history', 'transaction_history_paginated', 'travel', 'travel_plans_previous', 'trust', 'united-states', 'unread', 'update_cached', 'update_country_of_residence', 'update_friends_display', 'update_hide_from_search_engines', 'update_message', 'update_notifications', 'update_reservation_requirements', 'upload', 'uptodate', 'views_campaign', 'webcam_upload', 'weibo_signup_referral_finish', 'why_host', 'widget', 'wishlists', 'zendesk_login_jwt', 'Blackberry', 'Chromebook', 'Linux Desktop', 'Opera Phone', 'Windows Phone', 'iPodtouch', 'isgender_OTHER', 'issignup_method_google', 'issignup_flow_8', 'islanguage_cs', 'islanguage_da', 'islanguage_de', 'islanguage_el', 'islanguage_es', 'islanguage_fi', 'islanguage_fr', 'islanguage_hu', 'islanguage_is', 'islanguage_it', 'islanguage_ja', 'islanguage_ko', 'islanguage_nl', 'islanguage_no', 'islanguage_pl', 'islanguage_pt', 'islanguage_ru', 'islanguage_sv', 'islanguage_th', 'islanguage_tr', 'islanguage_zh', 'isaffiliate_channel_remarketing', 'isaffiliate_provider_baidu', 'isaffiliate_provider_daum', 'isaffiliate_provider_email-marketing', 'isaffiliate_provider_facebook', 'isaffiliate_provider_facebook-open-graph', 'isaffiliate_provider_gsp', 'isaffiliate_provider_meetup', 'isaffiliate_provider_naver', 'isaffiliate_provider_padmapper', 'isaffiliate_provider_vast', 'isaffiliate_provider_yahoo', 'isaffiliate_provider_yandex', 'isfirst_affiliate_tracked_-1', 'isfirst_affiliate_tracked_localops', 'isfirst_affiliate_tracked_marketing', 'isfirst_affiliate_tracked_product', 'isfirst_device_type_AndroidTablet', 'isfirst_device_type_Desktop(Other)', 'isfirst_device_type_SmartPhone(Other)', 'isfirst_browser_AOLExplorer', 'isfirst_browser_AndroidBrowser', 'isfirst_browser_AppleMail', 'isfirst_browser_BlackBerryBrowser', 'isfirst_browser_Chromium', 'isfirst_browser_CoolNovo', 'isfirst_browser_Googlebot', 'isfirst_browser_IEMobile', 'isfirst_browser_IceWeasel', 'isfirst_browser_Iron', 'isfirst_browser_Maxthon', 'isfirst_browser_MobileFirefox', 'isfirst_browser_Opera', 'isfirst_browser_OperaMini', 'isfirst_browser_RockMelt', 'isfirst_browser_SeaMonkey', 'isfirst_browser_Silk', 'isfirst_browser_SiteKiosk', 'isfirst_browser_SogouExplorer', 'isfirst_browser_TenFourFox', 'isfirst_browser_TheWorldBrowser', 'isfirst_browser_wOSBrowser', 'isnum_AD_pending_2', 'isnum_AD_pending_3', 'isnum_AD_pending_4', 'isnum_AD_pending_5', 'isnum_AD_pending_6', 'isajax_google_translate_reviews_1.0', 'isajax_google_translate_reviews_2.0', 'isajax_google_translate_reviews_3.0', 'isajax_google_translate_reviews_4.0', 'isajax_google_translate_reviews_5.0', 'isajax_google_translate_reviews_6.0', 'isajax_google_translate_reviews_7.0', 'isajax_google_translate_reviews_8.0', 'isajax_google_translate_reviews_10.0', 'isajax_google_translate_reviews_11.0', 'isajax_google_translate_reviews_30.0', 'isnum_messages_6.0', 'isnum_messages_7.0', 'isnum_messages_8.0', 'isnum_messages_9.0', 'isnum_messages_10.0', 'isnum_messages_11.0', 'isnum_messages_12.0', 'isnum_messages_13.0', 'isnum_messages_14.0', 'isnum_messages_15.0', 'isnum_messages_16.0', 'isnum_messages_17.0', 'isnum_messages_18.0', 'isnum_messages_19.0', 'isnum_messages_20.0', 'isnum_messages_21.0', 'isnum_messages_22.0', 'isnum_messages_23.0', 'isnum_messages_24.0', 'isnum_messages_25.0', 'isnum_messages_26.0', 'isnum_messages_27.0', 'isnum_messages_28.0', 'isnum_messages_29.0', 'isnum_messages_30.0', 'isnum_messages_31.0', 'isnum_messages_32.0', 'isnum_messages_33.0', 'isnum_messages_35.0', 'isnum_messages_37.0', 'isnum_messages_40.0', 'isnum_messages_41.0', 'isnum_messages_42.0', 'isnum_messages_47.0', 'isage_bucket_100+', 'isage_bucket_65-69', 'isage_bucket_70-74', 'isage_bucket_75-79', 'isage_bucket_80-84', 'isage_bucket_85-89', 'isage_bucket_90-94', 'isage_bucket_95-99', 'isaction_bucket_411-492', 'isaction_bucket_493-574', 'isaction_bucket_575-656', 'isaction_bucket_657-738', 'isaction_bucket_739-820', 'isaction_bucket_821-902', 'isaction_bucket_903-984', 'isaction_bucket_985-1066', 'isaction_bucket_morethan1067']\n"
     ]
    }
   ],
   "source": [
    "# ##Need to work on this\n",
    "# #train.groupby(['about_us']).count()\n",
    "# #train.groupby(['acculynk_bin_check_success']).count().reset_index()\n",
    "# to_be_dropped = []\n",
    "# for col in train.ix[:, train.columns != 'country_destination'].columns:\n",
    "#     x = train[col].value_counts(normalize=True)[0]\n",
    "#     #print col, x\n",
    "#     if x >0.99:\n",
    "#         to_be_dropped.append(col)\n",
    "        \n",
    "# #    print col\n",
    "# #    print train[col].value_counts(normalize=True)\n",
    "# print 'Dropped', len(to_be_dropped), 'columns: ',to_be_dropped\n",
    "# train = train.drop(to_be_dropped,axis=1)\n",
    "# test = test.drop(to_be_dropped,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['about_us'].value_counts(normalize=True))# [0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#OR ENCODE THE DATASET - transfors strings into integers\n",
    "#If one  hot encoding was applied this step wont do anything\n",
    "##add country_dest filter - not working at the moment\n",
    "# class MyEncoder:\n",
    "#     labelencoder={}\n",
    "\n",
    "#     def encode(self, d, col, labelencoder={}):\n",
    "#         if col not in labelencoder:\n",
    "#             labelencoder[col] = {}\n",
    "#         result = []\n",
    "#         for i in d:\n",
    "#             if i not in labelencoder[col]:\n",
    "#                 labelencoder[col][i] = len(labelencoder[col])+1\n",
    "#                 result.append(labelencoder[col][i])\n",
    "#             else:\n",
    "#                 result.append(labelencoder[col][i])\n",
    "#         return result,labelencoder\n",
    "\n",
    "# encodings={}\n",
    "# encoder = MyEncoder()\n",
    "\n",
    "# #pprint_df(foo,10)\n",
    "# for col in train.columns:\n",
    "#     if train[col].dtypes == object :\n",
    "#         print 'Encoding: ',col\n",
    "#         train[col], encodings = encoder.encode(train[col].values,col,encodings)\n",
    "#         test[col],  encodings = encoder.encode(test[col].values,col,encodings)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train[train['country_destination'] != 'NDF'][0:100].to_csv('C:\\\\git\\\\Airbnb\\\\trainNoCSV.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################       SELECT K BEST FEARURES METHOD\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "def SelectKBestFeatures(train, target, k, debug=False):\n",
    "    k_best = SelectKBest(f_classif, k='all')# score all features    \n",
    "    k_best.fit(train, target) \n",
    "    scores = k_best.scores_     \n",
    "    unsorted_pairs = zip(train.columns, scores)# joins the labels with the values \n",
    "    sorted_pairs = list(reversed(sorted(unsorted_pairs, key=lambda x: x[1])))\n",
    "\n",
    "    k_best_features = dict(sorted_pairs[:k])\n",
    "    if debug:\n",
    "        print sorted_pairs\n",
    "        print \"{0} best features: {1}\\n\".format(k, k_best_features.keys())\n",
    "\n",
    "    return k_best_features\n",
    "# import operator\n",
    "# k_best_features\n",
    "# sorted_x  = sorted(k_best_features.items(), key=operator.itemgetter(1))\n",
    "# (sorted_x).reverse()\n",
    "# print sorted_x\n",
    "\n",
    "#CREATE LIST OF COMBINATIONS BETWEEN FEATURES\n",
    "#new_list = k_best_features.keys()\n",
    "#import itertools\n",
    "#all = []\n",
    "#for i in range(len(new_list)):\n",
    "#    if i !=0: #already looked at individual features on step 1. Start by 2X2\n",
    "#        all.extend([sorted(l) for l in itertools.combinations(new_list, i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnum_AT_booking_request_0 1077.4059948\n",
      "isnum_AT_booking_request_1 1077.4059948\n",
      "isage_bucket_-1 989.443022457\n",
      "PopInKtoUS 984.831965543\n",
      "PopInKtoGB 984.535632154\n",
      "PopInKtoCA 983.605339115\n",
      "PopInKtoAU 981.424585907\n",
      "PopInKtoFR 978.684418889\n",
      "PopInKtoNL 952.182235412\n",
      "PopInKtoDE 943.750706183\n",
      "PopInKtoPT 942.425798601\n",
      "PopInKtoIT 908.720259442\n",
      "PopInKtoES 899.7677045\n",
      "isnum_AD_pending_0 857.583626757\n",
      "isnum_AD_pending_1 792.292685375\n",
      "isnum_messages_69+Messages 652.187063015\n",
      "requested 626.838889236\n",
      "verify 519.830254662\n",
      "isgender_-unknown- 422.063096018\n",
      "confirm_email 270.359993306\n",
      "isnum_messages_1.0 246.893848849\n",
      "travel_plans_current 229.740820024\n",
      "pending 214.01835416\n",
      "at_checkpoint 175.623947073\n",
      "secs_elapsed 160.280057515\n",
      "isgender_FEMALE 153.716400511\n",
      "isage_bucket_25-29 147.617298091\n",
      "issignup_app_Web 147.118309256\n",
      "isfirst_browser_-unknown- 145.389501685\n",
      "isage_bucket_30-34 144.51121516\n",
      "identity 142.065549665\n",
      "isgender_MALE 135.1437891\n",
      "issignup_flow_0 134.613271295\n",
      "dashboard 132.017824524\n",
      "cancellation_policies 116.89174937\n",
      "kba_update 94.6070758857\n",
      "ajax_refresh_subtotal 89.3755290779\n",
      "ajax_image_upload 88.7656273734\n",
      "edit 88.0275134657\n",
      "complete_redirect 87.8834175301\n",
      "isfirst_device_type_MacDesktop 83.9372954673\n",
      "agree_terms_check 83.8888532339\n",
      "jumio_redirect 81.6901178849\n",
      "isfirst_device_type_iPhone 78.1032557987\n",
      "kba 77.3982604185\n",
      "connect 76.1754589138\n",
      "qt2 74.240440057\n",
      "issignup_app_iOS 71.3506822476\n",
      "request_new_confirm_email 69.465593697\n",
      "Mac Desktop 68.8921170632\n",
      "edit_verification 65.9701710223\n",
      "set_user 65.9181792968\n",
      "isnum_messages_2.0 61.722238418\n",
      "personalize 61.2683483424\n",
      "phone_number_widget 61.1921814238\n",
      "issignup_flow_25 59.659212516\n",
      "similar_listings 59.3065894083\n",
      "show_personalize 59.2478618544\n",
      "jumio_token 58.9392612896\n",
      "isage_bucket_35-39 58.5530115093\n",
      "populate_from_facebook 56.3476579564\n",
      "istranslate_0.0 55.3060595833\n",
      "ask_question 53.4575649148\n",
      "isfirst_browser_Chrome 52.5284599214\n",
      "isaffiliate_channel_content 50.9627509076\n",
      "create 50.499464575\n",
      "isnum_messages_3.0 49.5547595857\n",
      "num_actions 49.3042018642\n",
      "qt_reply_v2 49.246383733\n",
      "languages_multiselect 47.8262178498\n",
      "issignup_app_Android 47.0399421126\n",
      "impressions 46.470813018\n",
      "isage_bucket_20-24 45.2822564992\n",
      "isfirst_device_type_Other/Unknown 44.126658173\n",
      "this_hosting_reviews 43.2437291929\n",
      "callback 42.3326402068\n",
      "ajax_lwlb_contact 40.9053268353\n",
      "isnum_AD_pending_2 40.5791178976\n",
      "qt_with 39.0012615795\n",
      "itinerary 36.9316228761\n",
      "istranslate_1.0 33.9518757248\n",
      "index 32.6518596502\n",
      "profile_pic 30.2215708049\n",
      "issignup_flow_12 29.2015948966\n",
      "istranslate_14.0 27.1825721192\n",
      "isnum_messages_4.0 26.9443712293\n",
      "plaxo_cb 26.8539761381\n",
      "my 26.7678059629\n",
      "Windows Desktop 24.7117813129\n",
      "ajax_get_referrals_amt 24.6466785889\n",
      "search_results 24.2796619251\n",
      "recommended_listings 24.1507941753\n",
      "isnum_messages_5.0 24.0377487918\n",
      "reviews_new 23.931097461\n",
      "pay 23.9052547737\n",
      "isage_bucket_40-44 23.7097661342\n",
      "issignup_flow_23 23.6061143148\n",
      "manage_listing 23.4316654497\n",
      "isfirst_device_type_AndroidPhone 21.3113812953\n",
      "phone_verification_success 20.9390328426\n"
     ]
    }
   ],
   "source": [
    "######################SELECT K-BEST FEATURES - filtering out NDFs\n",
    "#Remove columns that we dont have data on the training side, otherweise it will messup the select best features\n",
    "#If I want to automate the removal, it has to be here; it cannot be before the df_all split; they wont be removed because there\n",
    "#are values on the test rows but we cant do anything with them because there is nothing on train\n",
    "\n",
    "import operator\n",
    "#k_best_features = SelectKBestFeatures(train[train['country_destination'] != 'NDF'].ix[:, train.columns != 'country_destination'],target[target != 'NDF'], 1000, False)\n",
    "k_best_features = SelectKBestFeatures(train.ix[:, train.columns != 'country_destination'],target, 100, False)\n",
    "sorted_x  = sorted(k_best_features.items(), key=operator.itemgetter(1))\n",
    "(sorted_x).reverse()\n",
    "\n",
    "#print sorted_x\n",
    "for a,b in sorted_x:\n",
    "     print a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################       GRIDSEARCHCV to check best models - each one can take hours even days to run\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def suggestRandomForestClassifierModel():\n",
    "    \n",
    "    forest_params = [{\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                    \"max_features\": range(1, 6),            \n",
    "                    \"max_depth\": range(2, 6),\n",
    "                    #\"min_samples_split\": sp_randint(1, 11),\n",
    "                    #\"min_samples_leaf\": sp_randint(1, 11),\n",
    "                    \"bootstrap\": [True, False]\n",
    "                    }]\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_estimators=50), forest_params) # , scoring='f1'\n",
    "    best = clf.fit(train, target)\n",
    "    print best.best_estimator_\n",
    "\n",
    "    \n",
    "def suggestKNeighborsClassifierModel():\n",
    "    \n",
    "    neighbor_params = [{'n_neighbors' : range(1,10)\n",
    "                           , 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                           , 'weights': ['uniform', 'distance']\n",
    "                           , 'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "                           , 'p' : range(1,10)\n",
    "                       }]\n",
    "    clf = GridSearchCV(KNeighborsClassifier(), neighbor_params) # , scoring='f1'\n",
    "    best = clf.fit(train, target)\n",
    "    print best.best_estimator_\n",
    "    \n",
    "\n",
    "def suggestXGBClassifierModel1():\n",
    "    xgb_params = [{  \n",
    "                  'learning_rate': [0.25,0.3,0.35]\n",
    "                 , 'n_estimators' : [20,30,40,45]\n",
    "                 , 'gamma' : range(0,3)\n",
    "                 , 'max_depth': range(4,6)\n",
    "                 , 'min_child_weight' : range (0,4)\n",
    "                 , 'max_delta_step': range (1,4)\n",
    "                 , 'subsample': [0.5,1]\n",
    "                 , 'colsample_bytree': [0.5,1]\n",
    "                }]\n",
    "    \n",
    "    clf = GridSearchCV(XGBClassifier(objective='multi:softprob'), xgb_params) #, scoring='f1'\n",
    "    best = clf.fit(train[k_best_features.keys()], target)\n",
    "    print best.best_estimator_\n",
    "\n",
    "\n",
    "def suggestXGBClassifierModel2F():\n",
    "    xgb_params = [{                   \n",
    "                  'gamma' : range(0,10)\n",
    "                 , 'max_depth': range(3,10)\n",
    "                 , 'min_child_weight' : range (0,10)\n",
    "                 , 'max_delta_step': range (1,10)\n",
    "                 , 'subsample': [0.4,0.5,0.6,1]\n",
    "                 , 'colsample_bytree': [0.4,0.5,0.6,1]\n",
    "                }]\n",
    "\n",
    "    clf = GridSearchCV(XGBClassifier(learning_rate=0.3, n_estimators=25,objective='multi:softprob') \n",
    "                       , xgb_params) # , scoring='f1'\n",
    "\n",
    "\n",
    "    best = clf.fit(train, target)\n",
    "    print best.best_estimator_\n",
    "\n",
    "\n",
    "model2 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)      \n",
    "    \n",
    "def suggestXGBClassifierModel2(train,target):\n",
    "    xgb_params = [{                   \n",
    "#                    'colsample_bylevel': [0.5, 1]\n",
    "#                  , 'colsample_bytree':  [0.5, 1]\n",
    "                   'gamma' : range(1,3)\n",
    "                 , 'learning_rate' : [0.3, 0.4]\n",
    "                 , 'max_delta_step': range (1,3)\n",
    "                 , 'max_depth': range(3,6)\n",
    "                 , 'min_child_weight' : range (0,2)\n",
    "                 , 'n_estimators' : [30,40]\n",
    "                 , 'subsample': [0.5,1]\n",
    "                }]\n",
    "\n",
    "    clf = GridSearchCV(XGBClassifier(objective='binary:logistic'), xgb_params) # , scoring='f1'\n",
    "    \n",
    "    for col in target.columns:\n",
    "        if col != 'country_destination': \n",
    "            print 'Testing', col\n",
    "            best = clf.fit(train, target[col])\n",
    "            \n",
    "            f = open('C:\\\\git\\\\Airbnb\\\\GridSearch\\\\'+col+'.txt', 'w')\n",
    "            f.write(col)\n",
    "            f.write('\\n')\n",
    "            f.write(str(best.best_estimator_))\n",
    "            f.close()\n",
    "   \n",
    "#footrain =  train.ix[:, train.columns != 'country_destination'][:1000]\n",
    "#footarget = dfpivottarget[:1000]\n",
    "\n",
    "#print \"Start\" + time.strftime(\"%c\")    \n",
    "#suggestXGBClassifierModel2( train.ix[:, train.columns != 'country_destination'], dfpivottarget)\n",
    "#print \"End\" + time.strftime(\"%c\")\n",
    "#suggestXGBClassifierModel3( train.ix[:, train.columns != 'country_destination'], dfpivottarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "class XGBoostClassifierForGRIDSEARCHCV():\n",
    "    def __init__(self, num_boost_round=10, **params):\n",
    "        self.clf = None\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.params = params\n",
    "        self.params.update({'objective': 'binary:logistic'})\n",
    " \n",
    "    def fit(self, X, y, num_boost_round=None):\n",
    "        num_boost_round = num_boost_round or self.num_boost_round\n",
    "        self.label2num = dict((label, i) for i, label in enumerate(sorted(set(y))))\n",
    "        dtrain = xgb.DMatrix(X, label=[self.label2num[label] for label in y])\n",
    "        self.clf = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=num_boost_round)\n",
    " \n",
    "    def predict(self, X):\n",
    "        num2label = dict((i, label)for label, i in self.label2num.items())\n",
    "        Y = self.predict_proba(X)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        return np.array([num2label[i] for i in y])\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self.clf.predict(dtest)\n",
    " \n",
    "    def score(self, X, y):\n",
    "        Y = self.predict_proba(X)\n",
    "        return 1 / logloss(y, Y)\n",
    " \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        if 'num_boost_round' in params:\n",
    "            self.num_boost_round = params.pop('num_boost_round')\n",
    "        if 'objective' in params:\n",
    "            del params['objective']\n",
    "        self.params.update(params)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "def logloss(y_true, Y_pred):\n",
    "    label2num = dict((name, i) for i, name in enumerate(sorted(set(y_true))))\n",
    "    return -1 * sum(math.log(y[label2num[label]]) if y[label2num[label]] > 0 else -np.inf for y, label in zip(Y_pred, y_true)) / len(Y_pred)\n",
    "\n",
    "\n",
    "clf = XGBoostClassifierForGRIDSEARCHCV(\n",
    "    eval_metric = 'ndcg',\n",
    "    num_class = 2,\n",
    "    num_boost_round = 80,\n",
    "    max_depth = 12,\n",
    "    subsample = 0.5,\n",
    "    colsample_bytree = 1.0,\n",
    "    silent = 1\n",
    "    )\n",
    "parameters = {\n",
    "    'num_boost_round': [100, 250, 500],\n",
    "    'eta': [0.1, 0.3],\n",
    "    'max_depth': [6, 9, 12],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.9, 1.0],\n",
    "}\n",
    "clf = GridSearchCV(clf, parameters, cv=2)\n",
    "\n",
    "   \n",
    "# for col in dfpivottarget.columns:\n",
    "#     if col != 'country_destination': \n",
    "#     #if col == 'US':  \n",
    "#         f = open('C:\\\\git\\\\Airbnb\\\\GridSearch\\\\XGB'+col+'.txt', 'w')  \n",
    "#         f.write(col)\n",
    "#         f.write('\\n')\n",
    "#         print 'Testing', col\n",
    "#         clf.fit(train.ix[:, train.columns != 'country_destination'].values , dfpivottarget[col].values)\n",
    "#         best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "#         print(score)\n",
    "#         f.write(str(score))\n",
    "#         f.write('\\n')         \n",
    "#         for param_name in sorted(best_parameters.keys()):\n",
    "#             print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "#             f.write(str(\"%s: %r\" % (param_name, best_parameters[param_name])))\n",
    "#             f.write('\\n')            \n",
    "#         f.close()\n",
    "#https://github.com/dmlc/xgboost/blob/master/doc/parameter.md        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eta', 'max_depth']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.95324, std: 0.00320, params: {'eta': 0.1},\n",
       " mean: 0.75452, std: 0.00515, params: {'eta': 0.3}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SCORE AND CROSS VALIDATE\n",
    "#This methos applies cross validation on the train set. It is configured to always do on two ways refered as model1 and model2\n",
    "#mode1 (M1) and model2 (M2F) dont actually refer to the model itself but he way the model is being evaluated. \n",
    "#model 2 itself is also computed in 2 ways, full dataset (M2F) and noNDF (just M2)\n",
    "#on model1: the prediction is in one categorical column; on model2 each possible prediction value corresponds to a dummy colum\n",
    "#so it fits N model on the same dataset, N being the number of possible categories\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "def crossValidate(model1, model2, xgbparams, trainCV, targetCV, nfolds, debug, runM1=False, runM2=False, runM2F=False, runM3=False):\n",
    "    print \"Cross Validating...\"\n",
    "    cv = StratifiedShuffleSplit(targetCV['country_destination'], nfolds, test_size  = 0.3, random_state = 42)\n",
    "\n",
    "    foldercount=0\n",
    "    list_scores1 = [] \n",
    "    list_scores2 = [] \n",
    "    list_scores2F= [] \n",
    "    list_scores3= [] \n",
    "    avgscore1 = 0\n",
    "    avgscore2 = 0\n",
    "    avgscore2F = 0    \n",
    "    avgscore3 = 0  \n",
    "    \n",
    "    for train_index, test_index in cv: \n",
    "        if debug:\n",
    "            print 'Go for fold: ',foldercount    \n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)  \n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        test_features = []\n",
    "        test_labels = []    \n",
    "                \n",
    "        train_features, train_labels = trainCV.iloc[train_index], targetCV.iloc[train_index]\n",
    "        test_features, test_labels =   trainCV.iloc[test_index],  targetCV.iloc[test_index]        \n",
    "          \n",
    "        \n",
    "        #create an array from a <class 'pandas.core.series.Series'> to attch to the final DF - mainly for debugging\n",
    "        truth = []\n",
    "        for i in test_labels['country_destination']:\n",
    "            truth.append(i)\n",
    "        \n",
    "        #these are for the model being trained without NDF, so I have to filter the NDF values and the NDF column\n",
    "        train_featuresNoNDF = train_features[train_features['country_destination'] != 'NDF'].ix[:, train_features.columns != 'country_destination']\n",
    "        test_featuresNoNDF = test_features.ix[:, test_features.columns != 'country_destination']\n",
    "               \n",
    "        #this is for M1 and M2F  so i just remove the NDF column that was passed in for the previous filter\n",
    "        train_features = train_features.ix[:, train_features.columns != 'country_destination']\n",
    "        test_features = test_features.ix[:, test_features.columns != 'country_destination']\n",
    "\n",
    "        \n",
    "######## MODEL1 #####################################\n",
    "        if runM1:\n",
    "            model1.fit(train_featuresNoNDF, train_labels[train_labels['country_destination'] != 'NDF']['country_destination']) #train_labels['country_destination']\n",
    "            pred_array = model1.predict_proba(test_featuresNoNDF)     \n",
    "\n",
    "\n",
    "            my_debugger = []\n",
    "            list_of_labels1 = []        \n",
    "            for idx, val in enumerate(pred_array):\n",
    "                mypred = (zip(model1.classes_, pred_array[idx]))            \n",
    "                sorted_by_pred = sorted(mypred, key=lambda tup: tup[1], reverse=True)\n",
    "                sorted_by_pred = sorted_by_pred[:5]                      \n",
    "                only_labels =  [x[0] for x in sorted_by_pred]            \n",
    "                #apply prob correction?\n",
    "\n",
    "                list_of_labels1.append(only_labels)\n",
    "                if debug:\n",
    "                    my_debugger.append ([x[0] +': '+ str(x[1]) for x in sorted_by_pred])        \n",
    "\n",
    "\n",
    "            #list_of_labels contains a list where each element is a list with the top5 predictions for that ID, for example:\n",
    "            #[['NDF', 'US', 'other', 'ES', 'NL'], ['NDF', 'US', 'other', 'FR', 'IT'],]         \n",
    "            preds_df = pd.DataFrame(list_of_labels1, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))        \n",
    "            scores1 = score_predictions(preds_df, truth)        \n",
    "            avgscore1 = np.average(scores1)\n",
    "            list_scores1.append(avgscore1)       \n",
    "\n",
    "        \n",
    "        #This is used by M2 and M2F:\n",
    "        #get the ids from the original train ids based on the indexes we are testing:\n",
    "        ids = []\n",
    "        for i in train_ids.iloc[test_index]: # train_ids\n",
    "            ids.append(i)    \n",
    "            \n",
    "            \n",
    "######## MODEL2 #####################################\n",
    "        if runM2:\n",
    "            #pred_list contains the country and a list of probabilities the row belong to that country for that country\n",
    "            #1. Do the prediction for each country\n",
    "            pred_list = []\n",
    "            for col in targetCV.columns:\n",
    "                if col != 'country_destination' and col != 'NDF': #we dont want to predict NDF on this               \n",
    "                    model2.fit(train_featuresNoNDF, train_labels[train_labels['country_destination'] != 'NDF'][col])\n",
    "                    pred = model2.predict_proba(test_featuresNoNDF) \n",
    "                    pred_list.append([col, pred])\n",
    "\n",
    "\n",
    "            #2.Create the result data frame\n",
    "            dfResult = pd.DataFrame()               \n",
    "            dfResult['id'] = ids\n",
    "\n",
    "\n",
    "            for a in pred_list: # gets the predictions, one country at a time\n",
    "                dest = a[0] #get country name\"\n",
    "                predictions = a[1] #Get the \"True\" prediction. logreg.classes_ should return #array([False,  True], dtype=bool)  \n",
    "                values = []\n",
    "                for p in predictions:\n",
    "                    values.append(p[1])\n",
    "                dfResult[dest] = values                        \n",
    "            #at this point, dfResult contains one row per ID with one colum for each country and \n",
    "            #the True prediction probability for that country. \n",
    "\n",
    "            #The following Lines have been deprecated and subistituded by the get_top5_on_lis_of_labels3 function\n",
    "            #Unpivot: #result = unpivot_from_resultdf_to_dict(dfResult)\n",
    "            #Get topp5: #list_of_labels = pivot_back_with_topn(result,5, False, dfResult)        \n",
    "            #return dfResult\n",
    "            #function \"2\" applies the \"NDF correction\" because NDF wasn't one of the predictions\n",
    "            #return dfResult\n",
    "            list_of_labels2 = get_top5_on_lis_of_labels2(dfResult, False)\n",
    "\n",
    "            preds_df = pd.DataFrame(list_of_labels2, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))        \n",
    "            scores2 = score_predictions(preds_df, truth)\n",
    "            avgscore2 = np.average(scores2)\n",
    "            list_scores2.append(avgscore2)  \n",
    "\n",
    "######## MODEL2F #####################################\n",
    "#pretty much the same as the previous model - except fot the function being called \n",
    "#TODO: maybe unify the code in one method\n",
    "        if runM2F:\n",
    "            pred_listF = []\n",
    "            for col in targetCV.columns:\n",
    "                if col != 'country_destination': \n",
    "                    model2.fit(train_features, train_labels[col])\n",
    "                    pred = model2.predict_proba(test_features) \n",
    "                    pred_listF.append([col, pred])\n",
    "\n",
    "\n",
    "            #2.Create the result data frame\n",
    "            dfResult2F = pd.DataFrame()                \n",
    "            dfResult2F['id'] = ids# can use the same ids created on the previous step\n",
    "\n",
    "            for a in pred_listF: # gets the predictions, one country at a time\n",
    "                dest = a[0] #get country name\"\n",
    "                predictions = a[1] #Get the \"True\" prediction. logreg.classes_ should return #array([False,  True], dtype=bool)  \n",
    "                values = []\n",
    "                for p in predictions:\n",
    "                    values.append(p[1])\n",
    "                dfResult2F[dest] = values                \n",
    "        \n",
    "            list_of_labels2F = get_top5_on_lis_of_labels2F(dfResult2F, False)   \n",
    "\n",
    "\n",
    "            preds_df2F = pd.DataFrame(list_of_labels2F, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))        \n",
    "            scores2F = score_predictions(preds_df2F, truth)\n",
    "            avgscore2F = np.average(scores2F)\n",
    "            list_scores2F.append(avgscore2F)          \n",
    "\n",
    "######## MODEL3 #####################################\n",
    "        if runM3:\n",
    "            pred_listM3 = []\n",
    "            for col in targetCV.columns:\n",
    "                if col != 'country_destination':\n",
    "                    xgbmodel = xgb.train(params=xgbparams, dtrain=xgb.DMatrix(train_features.values, train_labels[col].values), num_boost_round=100)                    \n",
    "                    pred = xgbmodel.predict(xgb.DMatrix(test_features.values)) \n",
    "                    pred_listM3.append([col, pred])\n",
    "\n",
    "            #2.Create the result data frame\n",
    "            dfResult3 = pd.DataFrame()               \n",
    "            dfResult3['id'] = ids\n",
    "\n",
    "\n",
    "            for a in pred_listM3: # gets the predictions, one country at a time\n",
    "                dest = a[0] #get country name\"\n",
    "                predictions = a[1] #Get the \"True\" prediction. logreg.classes_ should return #array([False,  True], dtype=bool)  \n",
    "                values = []\n",
    "                for p in predictions:\n",
    "                    values.append(p[1])\n",
    "                dfResult3[dest] = values     \n",
    "                \n",
    "            list_of_labels3 = get_top5_on_lis_of_labels2F(dfResult3, False)\n",
    "\n",
    "            preds_df3 = pd.DataFrame(list_of_labels3, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))        \n",
    "            scores3 = score_predictions(preds_df3, truth)\n",
    "            avgscore3 = np.average(scores3)\n",
    "            list_scores3.append(avgscore3)              \n",
    "\n",
    "        \n",
    "##################\n",
    "        if debug:\n",
    "            print 'fold:', foldercount, 'score:', avgscore1,'|',avgscore2,'|',avgscore2F, '|',avgscore3\n",
    "            ################MODEL1 DEBUG\n",
    "            if runM1:\n",
    "                debbuger_df = pd.DataFrame(my_debugger, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))   \n",
    "                debbuger_df['t']  = truth            \n",
    "                debbuger_df['score']  = scores1\n",
    "                debbuger_df['id'] = ids        \n",
    "                debbuger_df.to_csv('C:\\\\git\\\\Airbnb\\\\LOG_EvalModel1__'+str(foldercount)+'.csv',index=False)   \n",
    "\n",
    "            ################MODEL2 DEBUG\n",
    "            if runM2:\n",
    "                dfResult['t']  = truth            \n",
    "                dfResult['score']  = scores2\n",
    "                foo = pd.concat((dfResult, preds_df), axis=1, ignore_index=False)\n",
    "                foo.to_csv('C:\\\\git\\\\Airbnb\\\\LOG_EvalModel2__'+str(foldercount)+'.csv',index=False)\n",
    "            \n",
    "            ################MODEL2F DEBUG\n",
    "            if runM2F:\n",
    "                dfResult2F['t']  = truth            \n",
    "                dfResult2F['score']  = scores2F\n",
    "                foo = pd.concat((dfResult2F, preds_df2F), axis=1, ignore_index=False)\n",
    "                foo.to_csv('C:\\\\git\\\\Airbnb\\\\LOG_EvalModel2F__'+str(foldercount)+'.csv',index=False)\n",
    "\n",
    "            ################MODEL3 DEBUG\n",
    "            if runM3:\n",
    "                dfResult3['t']  = truth            \n",
    "                dfResult3['score']  = scores3\n",
    "                foo = pd.concat((dfResult3, preds_df3), axis=1, ignore_index=False)\n",
    "                foo.to_csv('C:\\\\git\\\\Airbnb\\\\LOG_EvalModel3__'+str(foldercount)+'.csv',index=False)                \n",
    "            \n",
    "        foldercount +=1\n",
    "\n",
    "    return np.average(list_scores1), np.average(list_scores2), np.average(list_scores2F), np.average(list_scores3)\n",
    "\n",
    "# model1 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "#        gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "#        min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "#        objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
    "#        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "# model2 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "#        gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "#        min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "#        objective='binary:logistic', reg_alpha=0, reg_lambda=1,       \n",
    "#        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "# xgbparams = {'eta': 0.2,\n",
    "#       'max_depth': 6,\n",
    "#       'subsample': 0.5,\n",
    "#       'colsample_bytree': 0.5,\n",
    "#       'objective': 'binary:logistic',\n",
    "#       'num_class': 2}  \n",
    "\n",
    "# #features =  train.ix[:, train.columns != 'country_destination'].columns  #k_best_features.keys(\n",
    "# #features = train.columns\n",
    "# #train = train[:30000]\n",
    "# #dfpivottarget = dfpivottarget[:30000]\n",
    "\n",
    "# mycolumns = ['feature_list', 'scoreM1', 'ScoreM2NoNDF', 'ScoreM2Full', 'ScoreM3']\n",
    "# resultdf2 = pd.DataFrame(columns=mycolumns)\n",
    "\n",
    "# features = train [ [col for col in train.columns if col not in ['start_from_zero', \n",
    "#                                                                 'isstart_from_zero_0', 'isstart_from_zero_1',\n",
    "#                                                                 'num_actions', ]] ].columns\n",
    " \n",
    "# score1, ScoreM2NoNDF, ScoreM2Full, Score3 = crossValidate(model1, model2, xgbparams, train[features], dfpivottarget, \n",
    "#                                                           nfolds = 1, \n",
    "#                                                           debug= True,\n",
    "#                                                           runM1=False, \n",
    "#                                                           runM2=False, \n",
    "#                                                           runM2F=True, \n",
    "#                                                           runM3=False)\n",
    "\n",
    "# resultdf2.loc[len(resultdf2)] = (features, score1, ScoreM2NoNDF, ScoreM2Full, Score3)\n",
    "# resultdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "#################################################DEBUG################################################################\n",
    "######################################################################################################################\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "trainCV = train\n",
    "targetCV = dfpivottarget\n",
    "\n",
    "cv = StratifiedShuffleSplit(targetCV['country_destination'], 1, test_size  = 0.3, random_state = 42)\n",
    "\n",
    "foldercount=0\n",
    "list_scores1 = [] \n",
    "list_scores2 = [] \n",
    "list_scores2F= [] \n",
    "avgscore1 = 0\n",
    "avgscore2 = 0\n",
    "avgscore2F = 0    \n",
    "params  = {'eta': 0.1,\n",
    "          'max_depth': 6,\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 0.9,\n",
    "          'objective': 'binary:logistic',\n",
    "          'num_class': 2\n",
    "          } \n",
    "\n",
    "for train_index, test_index in cv: \n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []    \n",
    "\n",
    "    train_features, train_labels = trainCV.iloc[train_index], targetCV.iloc[train_index]\n",
    "    test_features, test_labels =   trainCV.iloc[test_index],  targetCV.iloc[test_index]        \n",
    "\n",
    "    #create an array from a <class 'pandas.core.series.Series'> to attch to the final DF - mainly for debugging\n",
    "    truth = []\n",
    "    for i in test_labels['country_destination']:\n",
    "        truth.append(i)\n",
    "\n",
    "    #this is for M1 and M2F  so i just remove the NDF column that was passed in for the previous filter\n",
    "    train_features = train_features.ix[:, train_features.columns != 'country_destination']\n",
    "    test_features = test_features.ix[:, test_features.columns != 'country_destination']\n",
    "\n",
    "    #pred_list contains the country and a list of probabilities the row belong to that country for that country\n",
    "    #1. Do the prediction for each country\n",
    "    #model2.fit(train_features, train_labels[col])\n",
    "    #pred = model2.predict_proba(test_features) \n",
    "    #pred_listF.append([col, pred])\n",
    "        \n",
    "        \n",
    "    pred_list = []\n",
    "    for col in targetCV.columns:\n",
    "        if col != 'country_destination':\n",
    "            X = train_features.values            \n",
    "            y = train_labels[col].values\n",
    "            \n",
    "            #print test_features\n",
    "            clf1 = xgb.train(params=params, dtrain=xgb.DMatrix(X, y), num_boost_round=100)                    \n",
    "            pred = clf1.predict(xgb.DMatrix(test_features.values)) \n",
    "            pred_list.append([col, pred])\n",
    "\n",
    "    ids = []\n",
    "    for i in train_ids.iloc[test_index]: # train_ids\n",
    "        ids.append(i)\n",
    "        \n",
    "    #2.Create the result data frame\n",
    "    dfResult = pd.DataFrame()               \n",
    "    dfResult['id'] = ids\n",
    "\n",
    "\n",
    "    for a in pred_list: # gets the predictions, one country at a time\n",
    "        dest = a[0] #get country name\"\n",
    "        predictions = a[1] #Get the \"True\" prediction. logreg.classes_ should return #array([False,  True], dtype=bool)  \n",
    "        values = []\n",
    "        for p in predictions:\n",
    "            values.append(p[1])\n",
    "        dfResult[dest] = values                        \n",
    "\n",
    "    list_of_labels2 = get_top5_on_lis_of_labels2F(dfResult, False)\n",
    "\n",
    "    preds_df = pd.DataFrame(list_of_labels2, columns=('pred1', 'pred2', 'pred3', 'pred4', 'pred5'))        \n",
    "    scores2 = score_predictions(preds_df, truth)\n",
    "    avgscore2 = np.average(scores2)\n",
    "    list_scores2.append(avgscore2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60919112,  0.61028111],\n",
       "       [ 0.59411204,  0.62901479],\n",
       "       [ 0.60409683,  0.60948884],\n",
       "       ..., \n",
       "       [ 0.58081907,  0.61655861],\n",
       "       [ 0.63605601,  0.47351736],\n",
       "       [ 0.65464675,  0.61473066]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "#################################################DEBUG################################################################\n",
    "######################################################################################################################\n",
    "X = train_features.values            \n",
    "y = train_labels['AU'].values        \n",
    "clf1 = xgb.train(params=params, dtrain=xgb.DMatrix(X, y), num_boost_round=10)                    \n",
    "foo =clf1.predict(xgb.DMatrix(test_features.values))    \n",
    "a = np.asarray(foo)\n",
    "np.savetxt(\"C:\\\\git\\\\Airbnb\\\\fooarray.csv\", a, delimiter=\",\")\n",
    "\n",
    "pred_listF= []\n",
    "model2 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,       \n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "\n",
    "pred_listF = []\n",
    "for col in targetCV.columns:\n",
    "    if col == 'AU': \n",
    "        model2.fit(train_features, train_labels[col])\n",
    "        pred = model2.predict_proba(test_features) \n",
    "        pred_listF.append([col, pred])\n",
    "        \n",
    "dfResult2F = pd.DataFrame()                \n",
    "dfResult2F['id'] = ids# can use the same ids created on the previous step\n",
    "\n",
    "for a in pred_listF: # gets the predictions, one country at a time\n",
    "    dest = a[0] #get country name\"\n",
    "    predictions = a[1] #Get the \"True\" prediction. logreg.classes_ should return #array([False,  True], dtype=bool)  \n",
    "    values = []\n",
    "    for p in predictions:\n",
    "        values.append(p[1])\n",
    "    dfResult2F[dest] = values                \n",
    "\n",
    "#list_of_labels2F = get_top5_on_lis_of_labels2F(dfResult2F, False)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AU', array([[  9.98124897e-01,   1.87509728e-03],\n",
       "         [  9.94539440e-01,   5.46054821e-03],\n",
       "         [  9.98789132e-01,   1.21086813e-03],\n",
       "         ..., \n",
       "         [  9.98976052e-01,   1.02392782e-03],\n",
       "         [  9.98894095e-01,   1.10592216e-03],\n",
       "         [  9.99362469e-01,   6.37520803e-04]], dtype=float32)]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_listF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndfTH = 0.2\n",
    "def get_top5_on_lis_of_labels2(dfResult, submission):\n",
    "    list_of_labels = []    \n",
    "\n",
    "    for _, row in dfResult.iterrows():\n",
    "        l = []\n",
    "        done = 0\n",
    "        pred_country = 'error'\n",
    "        i = 5\n",
    "        \n",
    "        \n",
    "        if max(row[1:11]) <= 0.35:\n",
    "            i = 4\n",
    "            if submission:\n",
    "                list_of_labels.append([row['id'], 'NDF'])\n",
    "            else:\n",
    "                l.append('NDF') \n",
    "\n",
    "        \n",
    "#         if row['AU'] < ndfTH and row['CA'] < ndfTH and row['DE'] < ndfTH and row['ES'] <ndfTH and row['FR'] < ndfTH and row['GB'] < ndfTH and row ['IT'] < ndfTH and row['NL'] < ndfTH and row['PT'] < ndfTH:\n",
    "#             if row['US'] < 0.5:\n",
    "#                 if row['other'] < 0.3:\n",
    "#                     pred_country = 'NDF'\n",
    "#                     done = 1\n",
    "        \n",
    "        \n",
    "#         if row['FR'] > 0.20:\n",
    "#             i = 4\n",
    "#             if submission:\n",
    "#                 list_of_labels.append([row['id'], 'FR'])\n",
    "#             else:\n",
    "#                 l.append('FR') \n",
    "            \n",
    "        for option in (\n",
    "                        map (\n",
    "                            it(0,1), row[1:].order(ascending=0)[:i].iteritems()\n",
    "                            )\n",
    "                      ):\n",
    "            country = option[0]\n",
    "            prob = option[1]\n",
    "            if done == 0:\n",
    "                pred_country = country\n",
    "                #if prob > 0.9 and country == 'US':\n",
    "                #    pred_country = country\n",
    "                #    done = 1                    \n",
    "                #if prob >= 0.8: \n",
    "                #    pred_country = country\n",
    "                #    done = 1\n",
    "                #if prob <= 0.3:                 \n",
    "                #    pred_country = 'NDF'\n",
    "                #    done = 1\n",
    "            if submission:\n",
    "                list_of_labels.append([row['id'], pred_country]) #for submissions\n",
    "            else:\n",
    "                l.append(pred_country)   #for train CV\n",
    "        \n",
    "        if submission == False:\n",
    "            list_of_labels.append(l)\n",
    "            \n",
    "    return list_of_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #simple tree for graph:\n",
    "# import pydot \n",
    "# from sklearn import tree\n",
    "# from sklearn.externals.six import StringIO  \n",
    "# features = ['PopInKtoAU','PopInKtoES','PopInKtoDE', 'at_checkpoint']\n",
    "\n",
    "\n",
    "# #train.columns\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(train[features], target)\n",
    "# tree.export_graphviz(clf,  out_file='C:\\\\git\\\\Airbnb\\\\t2.dot')\n",
    "\n",
    "\n",
    "# dot_data = StringIO() \n",
    "# tree.export_graphviz(clf, out_file=dot_data) \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "# graph.write_pdf(\"C:\\\\git\\\\Airbnb\\\\t2.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    i = 0\n",
    "    for feat in features:\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "        i = i + 1\n",
    "    outfile.close()\n",
    "\n",
    "create_feature_map(list(train.ix[:, train.columns != 'country_destination'].columns.values))\n",
    "importance = clf1.get_fscore(fmap='xgb.fmap')\n",
    "importance_df = pd.DataFrame(\n",
    "    sorted(importance.items(), key=operator.itemgetter(1)), \n",
    "    columns=['feature','fscore']\n",
    "    )    \n",
    "#importance_df.to_csv('C:\\\\git\\\\Airbnb\\\\importance_df.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
    "#            metric_params=None, n_neighbors=4, p=2, weights='uniform')\n",
    "\n",
    "# model = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "#               max_depth=7, max_features=9, max_leaf_nodes=None,\n",
    "#               min_samples_leaf=1, min_samples_split=2,\n",
    "#               min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
    "#               oob_score=False, random_state=None, verbose=0,\n",
    "#               warm_start=False)\n",
    "#model = RandomForestClassifier(n_estimators=50,  criterion='gini')\n",
    "#Best\n",
    "#model = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "#                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0) \n",
    "\n",
    "#model = XGBClassifier(max_depth=6, learning_rate=0.25, n_estimators=30,\n",
    "#                    objective='multi:softprob', subsample=0.7, colsample_bytree=0.6, seed=0)     \n",
    "\n",
    "model1 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "model2 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)     \n",
    "\n",
    "xgbparams = {'eta': 0.1,\n",
    "          'max_depth': 6,\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 0.9,\n",
    "          'objective': 'binary:logistic',\n",
    "          'num_class': 2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validating...\n",
      "Go for fold:  0\n",
      "fold: 0 score: 0 | 0 | 0.854961552739 | 0.819028063439\n",
      "Go for fold:  1\n",
      "fold: 1 score: 0 | 0 | 0.853833848079 | 0.819105522224\n",
      "Go for fold:  2\n",
      "fold: 2 score: 0 | 0 | 0.854957023789 | 0.818516099969\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_list</th>\n",
       "      <th>scoreM1</th>\n",
       "      <th>ScoreM2NoNDF</th>\n",
       "      <th>ScoreM2Full</th>\n",
       "      <th>ScoreM3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Index([u'about_us', u'account', u'acculynk_bin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.854584</td>\n",
       "      <td>0.818883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_list  scoreM1  ScoreM2NoNDF  \\\n",
       "0  Index([u'about_us', u'account', u'acculynk_bin...      NaN           NaN   \n",
       "\n",
       "   ScoreM2Full   ScoreM3  \n",
       "0     0.854584  0.818883  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN CROSS VALIDATION\n",
    "mycolumns = ['feature_list', 'scoreM1', 'ScoreM2NoNDF', 'ScoreM2Full', 'ScoreM3']\n",
    "resultdf2 = pd.DataFrame(columns=mycolumns)\n",
    "\n",
    "\n",
    "#features = train.columns# train.ix[:, train.columns != 'secs_elapsed'].columns #  #k_best_features.keys()\n",
    "features = train [ [col for col in train.columns if col not in ['start_from_zero', \n",
    "                                                                'isstart_from_zero_0', 'isstart_from_zero_1',\n",
    "                                                                'num_actions', ]] ].columns\n",
    " \n",
    "#features = k_best_features.keys()\n",
    "#features.append('country_destination')\n",
    "\n",
    "\n",
    "score1, ScoreM2NoNDF, ScoreM2Full, Score3 = crossValidate(model1, model2, xgbparams, train[features], dfpivottarget, \n",
    "                                                          nfolds = 3, \n",
    "                                                          debug= True,\n",
    "                                                          runM1=False, \n",
    "                                                          runM2=False, \n",
    "                                                          runM2F=True, \n",
    "                                                          runM3=True)\n",
    "\n",
    "resultdf2.loc[len(resultdf2)] = (features, score1, ScoreM2NoNDF, ScoreM2Full, Score3)\n",
    "resultdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#START ACTUAL PREDCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "#              max_depth=7, max_features=9, max_leaf_nodes=None,\n",
    "#              min_samples_leaf=1, min_samples_split=2,\n",
    "#              min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
    "#              oob_score=False, random_state=None, verbose=0,\n",
    "#              warm_start=False)\n",
    "\n",
    "# model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
    "#             metric_params=None, n_neighbors=4, p=2, weights='uniform')\n",
    "\n",
    "#model = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "#                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0) \n",
    "\n",
    "#model = XGBClassifier(max_depth=6, learning_rate=0.25, n_estimators=43,\n",
    "#                    objective='multi:softprob', subsample=0.6, colsample_bytree=0.6, seed=0) \n",
    "\n",
    "model1 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "model2 = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
    "       gamma=1, learning_rate=0.25, max_delta_step=2, max_depth=4,\n",
    "       min_child_weight=0, missing=None, n_estimators=40, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start01/07/16 18:32:53\n",
      "End1 01/07/16 18:32:56\n",
      "End2 01/07/16 18:32:56\n"
     ]
    }
   ],
   "source": [
    "############################### EVAL  MODEL1\n",
    "#features = train.columns#  k_best_features.keys() #\n",
    "\n",
    "\n",
    "model1.fit(train[features], target)\n",
    "pred_array = model1.predict_proba(test[features])\n",
    "\n",
    "\n",
    "list_of_labels1 = []\n",
    "print \"Start\" + time.strftime(\"%c\")\n",
    "for idx, val in enumerate(pred_array):\n",
    "    mypred = (zip(model1.classes_, pred_array[idx]))\n",
    "    sorted_by_pred = sorted(mypred, key=lambda tup: tup[1], reverse=True)\n",
    "    sorted_by_pred = sorted_by_pred[:5]\n",
    "    only_labels =  [x[0] for x in sorted_by_pred]\n",
    "    \n",
    "    #unpivot for submission:\n",
    "    for country in only_labels:   \n",
    "        list_of_labels1.append([test_ids[idx], country] )\n",
    "print \"End1 \" + time.strftime(\"%c\")        \n",
    "        \n",
    "preds_df = pd.DataFrame(list_of_labels1, columns=('id', 'country'))       \n",
    "print \"End2 \" + time.strftime(\"%c\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######EXPORT MODEL 1\n",
    "preds_df.to_csv('C:\\\\git\\\\Airbnb\\\\m1.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AU\n",
      "CA\n",
      "DE\n",
      "ES\n",
      "FR\n",
      "GB\n",
      "IT\n",
      "NDF\n",
      "NL\n",
      "other\n",
      "PT\n",
      "US\n"
     ]
    }
   ],
   "source": [
    "#######MODEL2\n",
    "#features = train.columns#  k_best_features.keys() #\n",
    "features = train.ix[:, train.columns != 'country_destination'].columns  #k_best_features.keys()\n",
    "Model2_list = []\n",
    "\n",
    "for col in dfpivottarget.columns:\n",
    "    if col != 'country_destination':\n",
    "        print col\n",
    "        model2.fit(train[features], dfpivottarget[col])\n",
    "        pred = model2.predict_proba(test[features])  \n",
    "\n",
    "        Model2_list.append([col, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model2_list.classes_ # check it is array([False,  True], dtype=bool)\n",
    "dfResult = pd.DataFrame()\n",
    "dfResult['id'] = test_ids\n",
    "\n",
    "for a in Model2_list:\n",
    "    dest = a[0] \n",
    "    predictions = a[1]\n",
    "    values = []\n",
    "    for p in predictions:\n",
    "        values.append(p[1])\n",
    "    dfResult[dest] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_labels2 = get_top5_on_lis_of_labels2F(dfResult, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(list_of_labels2, columns=('id', 'country'))\n",
    "preds_df.to_csv('C:\\\\git\\\\Airbnb\\\\foo.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AU</th>\n",
       "      <th>CA</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>IT</th>\n",
       "      <th>NL</th>\n",
       "      <th>other</th>\n",
       "      <th>PT</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>j5kxtvyc9b</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.018538</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.022586</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.077482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       AU        CA        DE        ES        FR        GB  \\\n",
       "0  j5kxtvyc9b  0.00064  0.000915  0.001786  0.007188  0.018538  0.009074   \n",
       "\n",
       "         IT        NL     other        PT        US  \n",
       "0  0.013322  0.001011  0.022586  0.001541  0.077482  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo =dfResult[dfResult['id']=='j5kxtvyc9b']\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['j5kxtvyc9b', 'US'],\n",
       " ['j5kxtvyc9b', 'other'],\n",
       " ['j5kxtvyc9b', 'FR'],\n",
       " ['j5kxtvyc9b', 'IT'],\n",
       " ['j5kxtvyc9b', 'GB']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top5_on_lis_of_labels2(foo, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B', 'C')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it(1,2)('ABCDEFG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = 'aabbcdd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
